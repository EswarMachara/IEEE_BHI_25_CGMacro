{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4fb305",
   "metadata": {},
   "source": [
    "# CGMacros Model Training and Evaluation\n",
    "\n",
    "This notebook implements the complete machine learning pipeline for predicting Carbohydrate Caloric Ratio (CCR) from multimodal CGMacros data.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Data loading and preprocessing\n",
    "2. Feature engineering\n",
    "3. Model training (baseline, intermediate, advanced)\n",
    "4. Model evaluation and comparison\n",
    "5. Results analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc65052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_loader import load_cgmacros_data\n",
    "from target import compute_ccr\n",
    "from feature_engineering import engineer_features, get_feature_summary\n",
    "from models import train_all_models, get_best_model\n",
    "from evaluation import evaluate_models\n",
    "from visualization import create_visualizations\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready to start the ML pipeline...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2fbef",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "print(\"Loading CGMacros data...\")\n",
    "data_dir = \"../data/raw\"\n",
    "raw_data = load_cgmacros_data(data_dir)\n",
    "\n",
    "print(f\"Raw data loaded: {raw_data.shape}\")\n",
    "print(f\"Columns: {len(raw_data.columns)}\")\n",
    "print(f\"Memory usage: {raw_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic info\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute target variable (CCR)\n",
    "print(\"Computing Carbohydrate Caloric Ratio (CCR)...\")\n",
    "data_with_target = compute_ccr(raw_data)\n",
    "\n",
    "print(f\"Data after CCR computation: {data_with_target.shape}\")\n",
    "\n",
    "# Check if CCR was successfully computed\n",
    "if 'ccr' in data_with_target.columns:\n",
    "    ccr_stats = data_with_target['ccr'].describe()\n",
    "    print(\"\\nCCR Statistics:\")\n",
    "    print(ccr_stats)\n",
    "    \n",
    "    # Quick visualization of CCR distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(data_with_target['ccr'].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('CCR Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('CCR Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(data_with_target['ccr'].dropna())\n",
    "    plt.ylabel('CCR Value')\n",
    "    plt.title('CCR Box Plot')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"WARNING: CCR could not be computed. Check if nutrient data is available.\")\n",
    "    print(\"Available columns:\", list(data_with_target.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5e5d4",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature engineering\n",
    "print(\"Starting feature engineering pipeline...\")\n",
    "features_df = engineer_features(data_with_target)\n",
    "\n",
    "print(f\"Features after engineering: {features_df.shape}\")\n",
    "\n",
    "# Get feature summary\n",
    "feature_summary = get_feature_summary(features_df)\n",
    "print(\"\\nFeature Engineering Summary:\")\n",
    "for key, value in feature_summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32525442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature categories\n",
    "feature_groups = feature_summary.get('feature_groups', {})\n",
    "\n",
    "if feature_groups:\n",
    "    # Visualize feature distribution by category\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    categories = list(feature_groups.keys())\n",
    "    counts = list(feature_groups.values())\n",
    "    \n",
    "    plt.bar(categories, counts, alpha=0.7, color='lightcoral')\n",
    "    plt.xlabel('Feature Category')\n",
    "    plt.ylabel('Number of Features')\n",
    "    plt.title('Feature Distribution by Category')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(i, count + 0.5, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFeature counts by category:\")\n",
    "    for category, count in feature_groups.items():\n",
    "        print(f\"  {category}: {count} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality after feature engineering\n",
    "print(\"Data quality assessment after feature engineering:\")\n",
    "\n",
    "# Missing data\n",
    "missing_pct = (features_df.isnull().sum() / len(features_df)) * 100\n",
    "high_missing = missing_pct[missing_pct > 10].sort_values(ascending=False)\n",
    "\n",
    "if len(high_missing) > 0:\n",
    "    print(f\"\\nFeatures with >10% missing data: {len(high_missing)}\")\n",
    "    print(high_missing.head(10))\n",
    "else:\n",
    "    print(\"\\nNo features with >10% missing data - excellent!\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nData types:\")\n",
    "print(features_df.dtypes.value_counts())\n",
    "\n",
    "# Memory usage\n",
    "memory_mb = features_df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nMemory usage: {memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2da32",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4932e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"Starting model training pipeline...\")\n",
    "print(\"This may take several minutes depending on data size...\")\n",
    "\n",
    "# Check if we have the target variable\n",
    "if 'ccr' not in features_df.columns:\n",
    "    print(\"ERROR: CCR target variable not found in features DataFrame\")\n",
    "    print(\"Available columns:\", list(features_df.columns))\n",
    "else:\n",
    "    # Train models\n",
    "    models_results = train_all_models(features_df, save_dir='../models')\n",
    "    \n",
    "    print(\"\\nModel training completed!\")\n",
    "    print(f\"Models trained: {len(models_results.get('models', {}))}\")\n",
    "    \n",
    "    # Display training results\n",
    "    evaluation_results = models_results.get('evaluation_results', {})\n",
    "    \n",
    "    if evaluation_results:\n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Model':<20} {'NRMSE':<10} {'Correlation':<12} {'R²':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for model_name, metrics in evaluation_results.items():\n",
    "            nrmse = metrics.get('nrmse', 'N/A')\n",
    "            corr = metrics.get('correlation', 'N/A')\n",
    "            r2 = metrics.get('r2', 'N/A')\n",
    "            \n",
    "            nrmse_str = f\"{nrmse:.4f}\" if isinstance(nrmse, (int, float)) else str(nrmse)\n",
    "            corr_str = f\"{corr:.4f}\" if isinstance(corr, (int, float)) else str(corr)\n",
    "            r2_str = f\"{r2:.4f}\" if isinstance(r2, (int, float)) else str(r2)\n",
    "            \n",
    "            print(f\"{model_name:<20} {nrmse_str:<10} {corr_str:<12} {r2_str:<10}\")\n",
    "        \n",
    "        # Find best model\n",
    "        best_model, best_score = get_best_model(evaluation_results, 'nrmse')\n",
    "        if best_model:\n",
    "            print(f\"\\nBest Model: {best_model} (NRMSE: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30a906",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "if 'models_results' in locals() and models_results:\n",
    "    print(\"Performing comprehensive model evaluation...\")\n",
    "    \n",
    "    comprehensive_results = evaluate_models(models_results, features_df)\n",
    "    \n",
    "    print(\"Evaluation completed!\")\n",
    "    \n",
    "    # Display evaluation summary\n",
    "    eval_summary = comprehensive_results.get('evaluation_summary', {})\n",
    "    \n",
    "    if eval_summary:\n",
    "        print(\"\\nEvaluation Summary:\")\n",
    "        print(f\"Total models evaluated: {eval_summary.get('total_models', 'N/A')}\")\n",
    "        \n",
    "        best_model_info = eval_summary.get('best_overall_model', {})\n",
    "        if best_model_info:\n",
    "            print(f\"Best overall model: {best_model_info.get('name', 'N/A')}\")\n",
    "            print(f\"Best NRMSE: {best_model_info.get('nrmse', 'N/A')}\")\n",
    "            print(f\"Best Correlation: {best_model_info.get('correlation', 'N/A')}\")\n",
    "        \n",
    "        key_findings = eval_summary.get('key_findings', [])\n",
    "        if key_findings:\n",
    "            print(\"\\nKey Findings:\")\n",
    "            for finding in key_findings:\n",
    "                print(f\"  • {finding}\")\n",
    "else:\n",
    "    print(\"Models not available for evaluation. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "if 'comprehensive_results' in locals():\n",
    "    model_metrics = comprehensive_results.get('model_metrics', {})\n",
    "    \n",
    "    if model_metrics:\n",
    "        # Create performance comparison plot\n",
    "        metrics_df = pd.DataFrame(model_metrics).T\n",
    "        \n",
    "        if not metrics_df.empty:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # NRMSE comparison\n",
    "            if 'nrmse' in metrics_df.columns:\n",
    "                axes[0, 0].bar(metrics_df.index, metrics_df['nrmse'], color='lightcoral', alpha=0.8)\n",
    "                axes[0, 0].set_title('NRMSE (Lower is Better)')\n",
    "                axes[0, 0].set_ylabel('NRMSE')\n",
    "                axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "                axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Correlation comparison\n",
    "            if 'correlation' in metrics_df.columns:\n",
    "                axes[0, 1].bar(metrics_df.index, metrics_df['correlation'], color='lightgreen', alpha=0.8)\n",
    "                axes[0, 1].set_title('Correlation (Higher is Better)')\n",
    "                axes[0, 1].set_ylabel('Correlation')\n",
    "                axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "                axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # R² comparison\n",
    "            if 'r2' in metrics_df.columns:\n",
    "                axes[1, 0].bar(metrics_df.index, metrics_df['r2'], color='lightblue', alpha=0.8)\n",
    "                axes[1, 0].set_title('R² Score (Higher is Better)')\n",
    "                axes[1, 0].set_ylabel('R²')\n",
    "                axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "                axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # MAE comparison\n",
    "            if 'mae' in metrics_df.columns:\n",
    "                axes[1, 1].bar(metrics_df.index, metrics_df['mae'], color='plum', alpha=0.8)\n",
    "                axes[1, 1].set_title('MAE (Lower is Better)')\n",
    "                axes[1, 1].set_ylabel('MAE')\n",
    "                axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "                axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8bbff",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "if 'comprehensive_results' in locals():\n",
    "    feature_importance = comprehensive_results.get('feature_importance', {})\n",
    "    \n",
    "    if feature_importance:\n",
    "        top_features = feature_importance.get('top_features', {})\n",
    "        \n",
    "        if 'mean_importance' in top_features:\n",
    "            # Plot top important features\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Sort features by importance\n",
    "            importance_items = list(top_features['mean_importance'].items())\n",
    "            importance_items.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Take top 15 features\n",
    "            top_15 = importance_items[:15]\n",
    "            features, importances = zip(*top_15)\n",
    "            \n",
    "            plt.barh(range(len(features)), importances, color='steelblue', alpha=0.8)\n",
    "            plt.yticks(range(len(features)), features)\n",
    "            plt.xlabel('Mean Feature Importance')\n",
    "            plt.title('Top 15 Most Important Features', fontsize=14, fontweight='bold')\n",
    "            plt.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"Top 10 Most Important Features:\")\n",
    "            for i, (feature, importance) in enumerate(top_15[:10], 1):\n",
    "                print(f\"{i:2d}. {feature:<30} {importance:.4f}\")\n",
    "        \n",
    "        # Category importance\n",
    "        category_importance = feature_importance.get('category_importance', {})\n",
    "        \n",
    "        if category_importance:\n",
    "            print(\"\\nFeature Importance by Category:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for category, info in category_importance.items():\n",
    "                mean_imp = info.get('mean_importance', 0)\n",
    "                feature_count = info.get('feature_count', 0)\n",
    "                top_feature = info.get('top_feature', 'N/A')\n",
    "                \n",
    "                print(f\"{category:<15} Mean: {mean_imp:.4f}, Count: {feature_count:2d}, Top: {top_feature}\")\n",
    "    else:\n",
    "        print(\"Feature importance data not available\")\n",
    "else:\n",
    "    print(\"Evaluation results not available. Please run the evaluation cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cac645",
   "metadata": {},
   "source": [
    "## 6. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs actual analysis for best model\n",
    "if 'models_results' in locals() and 'comprehensive_results' in locals():\n",
    "    trainer = models_results.get('trainer')\n",
    "    data_splits = models_results.get('data_splits', {})\n",
    "    \n",
    "    X_test = data_splits.get('X_test')\n",
    "    y_test = data_splits.get('y_test')\n",
    "    \n",
    "    if trainer and X_test is not None and y_test is not None:\n",
    "        # Get best model\n",
    "        eval_summary = comprehensive_results.get('evaluation_summary', {})\n",
    "        best_model_info = eval_summary.get('best_overall_model', {})\n",
    "        best_model_name = best_model_info.get('name')\n",
    "        \n",
    "        if best_model_name and best_model_name in trainer.models:\n",
    "            # Make predictions\n",
    "            y_pred = trainer.predict(best_model_name, X_test)\n",
    "            \n",
    "            # Create prediction analysis plot\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "            fig.suptitle(f'Prediction Analysis - {best_model_name.replace(\"_\", \" \").title()}', \n",
    "                        fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Predicted vs Actual scatter plot\n",
    "            axes[0].scatter(y_test, y_pred, alpha=0.6, s=50)\n",
    "            min_val = min(y_test.min(), y_pred.min())\n",
    "            max_val = max(y_test.max(), y_pred.max())\n",
    "            axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "            axes[0].set_xlabel('Actual CCR')\n",
    "            axes[0].set_ylabel('Predicted CCR')\n",
    "            axes[0].set_title('Predicted vs Actual')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].legend()\n",
    "            \n",
    "            # Residuals plot\n",
    "            residuals = y_test - y_pred\n",
    "            axes[1].scatter(y_pred, residuals, alpha=0.6)\n",
    "            axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "            axes[1].set_xlabel('Predicted CCR')\n",
    "            axes[1].set_ylabel('Residuals')\n",
    "            axes[1].set_title('Residuals vs Predicted')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Residuals histogram\n",
    "            axes[2].hist(residuals, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[2].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "            axes[2].set_xlabel('Residuals')\n",
    "            axes[2].set_ylabel('Frequency')\n",
    "            axes[2].set_title('Residuals Distribution')\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Prediction statistics\n",
    "            mse = np.mean(residuals**2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = np.mean(np.abs(residuals))\n",
    "            r2 = 1 - np.sum(residuals**2) / np.sum((y_test - y_test.mean())**2)\n",
    "            correlation = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "            \n",
    "            print(f\"\\nPrediction Statistics for {best_model_name}:\")\n",
    "            print(f\"RMSE: {rmse:.4f}\")\n",
    "            print(f\"MAE: {mae:.4f}\")\n",
    "            print(f\"R²: {r2:.4f}\")\n",
    "            print(f\"Correlation: {correlation:.4f}\")\n",
    "            print(f\"Mean Residual: {np.mean(residuals):.4f}\")\n",
    "            print(f\"Std Residual: {np.std(residuals):.4f}\")\n",
    "        else:\n",
    "            print(\"Best model not found or not available for prediction\")\n",
    "    else:\n",
    "        print(\"Test data not available for prediction analysis\")\n",
    "else:\n",
    "    print(\"Model results not available. Please run the training cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe02f5",
   "metadata": {},
   "source": [
    "## 7. Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "if 'features_df' in locals():\n",
    "    output_dir = Path('../data/processed')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    features_file = output_dir / 'features_with_target.csv'\n",
    "    features_df.to_csv(features_file, index=False)\n",
    "    print(f\"Processed features saved to: {features_file}\")\n",
    "\n",
    "# Generate visualizations\n",
    "if 'models_results' in locals() and 'comprehensive_results' in locals():\n",
    "    print(\"\\nGenerating comprehensive visualizations...\")\n",
    "    \n",
    "    results_dir = Path('../results')\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        create_visualizations(\n",
    "            features_df, \n",
    "            models_results, \n",
    "            comprehensive_results, \n",
    "            str(results_dir)\n",
    "        )\n",
    "        print(f\"Visualizations saved to: {results_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating visualizations: {str(e)}\")\n",
    "\n",
    "# Summary of outputs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'features_df' in locals():\n",
    "    print(f\"✓ Feature engineering completed: {features_df.shape[1]} features\")\n",
    "\n",
    "if 'models_results' in locals():\n",
    "    num_models = len(models_results.get('models', {}))\n",
    "    print(f\"✓ Model training completed: {num_models} models trained\")\n",
    "\n",
    "if 'comprehensive_results' in locals():\n",
    "    print(f\"✓ Model evaluation completed\")\n",
    "    \n",
    "    eval_summary = comprehensive_results.get('evaluation_summary', {})\n",
    "    best_model_info = eval_summary.get('best_overall_model', {})\n",
    "    \n",
    "    if best_model_info:\n",
    "        best_name = best_model_info.get('name', 'Unknown')\n",
    "        best_nrmse = best_model_info.get('nrmse', 'N/A')\n",
    "        best_corr = best_model_info.get('correlation', 'N/A')\n",
    "        \n",
    "        print(f\"✓ Best model: {best_name}\")\n",
    "        print(f\"  - NRMSE: {best_nrmse}\")\n",
    "        print(f\"  - Correlation: {best_corr}\")\n",
    "\n",
    "print(f\"✓ Results saved to: ../results/\")\n",
    "print(f\"✓ Models saved to: ../models/\")\n",
    "print(f\"✓ Processed data saved to: ../data/processed/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pipeline execution completed successfully!\")\n",
    "print(\"Check the results directory for detailed outputs.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
