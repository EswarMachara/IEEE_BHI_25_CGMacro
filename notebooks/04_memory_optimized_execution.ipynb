{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60909a8b",
   "metadata": {},
   "source": [
    "# Memory-Optimized CGMacros CCR Prediction Pipeline\n",
    "\n",
    "This notebook executes the complete pipeline with memory optimizations to handle the full dataset (687,580 records) efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9e42b",
   "metadata": {},
   "source": [
    "## Phase 0: Environment Setup and Memory Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef54c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 116.6 MB\n",
      "Available system memory: 2.9 GB\n",
      "✅ Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory monitoring function\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "print(f\"Initial memory usage: {get_memory_usage():.1f} MB\")\n",
    "print(f\"Available system memory: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeaf8f3",
   "metadata": {},
   "source": [
    "## Phase 1: Memory-Optimized Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bbda5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:52:14,681 - INFO - Loading CGMacros participant files with memory optimization...\n",
      "2025-10-02 12:52:14,681 - INFO - Found 45 participant files. Processing in chunks of 5...\n",
      "2025-10-02 12:52:14,681 - INFO - Processing chunk 1/9 (files 1-5)\n",
      "2025-10-02 12:52:14,681 - INFO - Found 45 participant files. Processing in chunks of 5...\n",
      "2025-10-02 12:52:14,681 - INFO - Processing chunk 1/9 (files 1-5)\n",
      "2025-10-02 12:52:14,800 - INFO - Loaded 14730 records for participant 1\n",
      "2025-10-02 12:52:14,800 - INFO - Loaded 14730 records for participant 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before data loading: 116.9 MB\n",
      "Loading CGMacros data with chunked processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:52:14,879 - INFO - Loaded 17025 records for participant 2\n",
      "2025-10-02 12:52:15,131 - INFO - Loaded 14565 records for participant 3\n",
      "2025-10-02 12:52:15,131 - INFO - Loaded 14565 records for participant 3\n",
      "2025-10-02 12:52:15,266 - INFO - Loaded 14275 records for participant 4\n",
      "2025-10-02 12:52:15,266 - INFO - Loaded 14275 records for participant 4\n",
      "2025-10-02 12:52:15,392 - INFO - Loaded 14460 records for participant 5\n",
      "2025-10-02 12:52:15,392 - INFO - Loaded 14460 records for participant 5\n",
      "2025-10-02 12:52:15,423 - INFO - Chunk 1 combined: 75055 records\n",
      "2025-10-02 12:52:15,470 - INFO - Processing chunk 2/9 (files 6-10)\n",
      "2025-10-02 12:52:15,423 - INFO - Chunk 1 combined: 75055 records\n",
      "2025-10-02 12:52:15,470 - INFO - Processing chunk 2/9 (files 6-10)\n",
      "2025-10-02 12:52:15,548 - INFO - Loaded 14460 records for participant 6\n",
      "2025-10-02 12:52:15,548 - INFO - Loaded 14460 records for participant 6\n",
      "2025-10-02 12:52:15,579 - INFO - Loaded 5655 records for participant 7\n",
      "2025-10-02 12:52:15,579 - INFO - Loaded 5655 records for participant 7\n",
      "2025-10-02 12:52:15,642 - INFO - Loaded 14760 records for participant 8\n",
      "2025-10-02 12:52:15,642 - INFO - Loaded 14760 records for participant 8\n",
      "2025-10-02 12:52:15,704 - INFO - Loaded 14370 records for participant 9\n",
      "2025-10-02 12:52:15,704 - INFO - Loaded 14370 records for participant 9\n",
      "2025-10-02 12:52:15,767 - INFO - Loaded 16155 records for participant 10\n",
      "2025-10-02 12:52:15,767 - INFO - Loaded 16155 records for participant 10\n",
      "2025-10-02 12:52:15,786 - INFO - Chunk 2 combined: 65400 records\n",
      "2025-10-02 12:52:15,833 - INFO - Processing chunk 3/9 (files 11-15)\n",
      "2025-10-02 12:52:15,786 - INFO - Chunk 2 combined: 65400 records\n",
      "2025-10-02 12:52:15,833 - INFO - Processing chunk 3/9 (files 11-15)\n",
      "2025-10-02 12:52:15,884 - INFO - Loaded 14805 records for participant 11\n",
      "2025-10-02 12:52:15,884 - INFO - Loaded 14805 records for participant 11\n",
      "2025-10-02 12:52:15,947 - INFO - Loaded 14655 records for participant 12\n",
      "2025-10-02 12:52:15,947 - INFO - Loaded 14655 records for participant 12\n",
      "2025-10-02 12:52:16,034 - INFO - Loaded 15840 records for participant 13\n",
      "2025-10-02 12:52:16,034 - INFO - Loaded 15840 records for participant 13\n",
      "2025-10-02 12:52:16,096 - INFO - Loaded 17250 records for participant 14\n",
      "2025-10-02 12:52:16,096 - INFO - Loaded 17250 records for participant 14\n",
      "2025-10-02 12:52:16,174 - INFO - Loaded 16875 records for participant 15\n",
      "2025-10-02 12:52:16,174 - INFO - Loaded 16875 records for participant 15\n",
      "2025-10-02 12:52:16,190 - INFO - Chunk 3 combined: 79425 records\n",
      "2025-10-02 12:52:16,234 - INFO - Processing chunk 4/9 (files 16-20)\n",
      "2025-10-02 12:52:16,190 - INFO - Chunk 3 combined: 79425 records\n",
      "2025-10-02 12:52:16,234 - INFO - Processing chunk 4/9 (files 16-20)\n",
      "2025-10-02 12:52:16,313 - INFO - Loaded 16290 records for participant 16\n",
      "2025-10-02 12:52:16,313 - INFO - Loaded 16290 records for participant 16\n",
      "2025-10-02 12:52:16,375 - INFO - Loaded 16185 records for participant 17\n",
      "2025-10-02 12:52:16,375 - INFO - Loaded 16185 records for participant 17\n",
      "2025-10-02 12:52:16,435 - INFO - Loaded 14085 records for participant 18\n",
      "2025-10-02 12:52:16,435 - INFO - Loaded 14085 records for participant 18\n",
      "2025-10-02 12:52:16,497 - INFO - Loaded 14430 records for participant 19\n",
      "2025-10-02 12:52:16,497 - INFO - Loaded 14430 records for participant 19\n",
      "2025-10-02 12:52:16,559 - INFO - Loaded 16260 records for participant 20\n",
      "2025-10-02 12:52:16,559 - INFO - Loaded 16260 records for participant 20\n",
      "2025-10-02 12:52:16,575 - INFO - Chunk 4 combined: 77250 records\n",
      "2025-10-02 12:52:16,622 - INFO - Processing chunk 5/9 (files 21-25)\n",
      "2025-10-02 12:52:16,575 - INFO - Chunk 4 combined: 77250 records\n",
      "2025-10-02 12:52:16,622 - INFO - Processing chunk 5/9 (files 21-25)\n",
      "2025-10-02 12:52:16,700 - INFO - Loaded 16125 records for participant 21\n",
      "2025-10-02 12:52:16,700 - INFO - Loaded 16125 records for participant 21\n",
      "2025-10-02 12:52:16,762 - INFO - Loaded 17625 records for participant 22\n",
      "2025-10-02 12:52:16,762 - INFO - Loaded 17625 records for participant 22\n",
      "2025-10-02 12:52:16,841 - INFO - Loaded 16185 records for participant 23\n",
      "2025-10-02 12:52:16,841 - INFO - Loaded 16185 records for participant 23\n",
      "2025-10-02 12:52:16,919 - INFO - Loaded 16320 records for participant 26\n",
      "2025-10-02 12:52:16,919 - INFO - Loaded 16320 records for participant 26\n",
      "2025-10-02 12:52:16,966 - INFO - Loaded 14535 records for participant 27\n",
      "2025-10-02 12:52:16,966 - INFO - Loaded 14535 records for participant 27\n",
      "2025-10-02 12:52:16,981 - INFO - Chunk 5 combined: 80790 records\n",
      "2025-10-02 12:52:17,028 - INFO - Processing chunk 6/9 (files 26-30)\n",
      "2025-10-02 12:52:16,981 - INFO - Chunk 5 combined: 80790 records\n",
      "2025-10-02 12:52:17,028 - INFO - Processing chunk 6/9 (files 26-30)\n",
      "2025-10-02 12:52:17,122 - INFO - Loaded 18735 records for participant 28\n",
      "2025-10-02 12:52:17,122 - INFO - Loaded 18735 records for participant 28\n",
      "2025-10-02 12:52:17,200 - INFO - Loaded 17730 records for participant 29\n",
      "2025-10-02 12:52:17,200 - INFO - Loaded 17730 records for participant 29\n",
      "2025-10-02 12:52:17,356 - INFO - Loaded 15465 records for participant 30\n",
      "2025-10-02 12:52:17,356 - INFO - Loaded 15465 records for participant 30\n",
      "2025-10-02 12:52:17,450 - INFO - Loaded 13725 records for participant 31\n",
      "2025-10-02 12:52:17,450 - INFO - Loaded 13725 records for participant 31\n",
      "2025-10-02 12:52:17,512 - INFO - Loaded 13785 records for participant 32\n",
      "2025-10-02 12:52:17,512 - INFO - Loaded 13785 records for participant 32\n",
      "2025-10-02 12:52:17,528 - INFO - Chunk 6 combined: 79440 records\n",
      "2025-10-02 12:52:17,575 - INFO - Processing chunk 7/9 (files 31-35)\n",
      "2025-10-02 12:52:17,528 - INFO - Chunk 6 combined: 79440 records\n",
      "2025-10-02 12:52:17,575 - INFO - Processing chunk 7/9 (files 31-35)\n",
      "2025-10-02 12:52:17,653 - INFO - Loaded 15840 records for participant 33\n",
      "2025-10-02 12:52:17,653 - INFO - Loaded 15840 records for participant 33\n",
      "2025-10-02 12:52:17,716 - INFO - Loaded 14700 records for participant 34\n",
      "2025-10-02 12:52:17,716 - INFO - Loaded 14700 records for participant 34\n",
      "2025-10-02 12:52:17,778 - INFO - Loaded 14400 records for participant 35\n",
      "2025-10-02 12:52:17,778 - INFO - Loaded 14400 records for participant 35\n",
      "2025-10-02 12:52:17,856 - INFO - Loaded 17115 records for participant 36\n",
      "2025-10-02 12:52:17,856 - INFO - Loaded 17115 records for participant 36\n",
      "2025-10-02 12:52:17,919 - INFO - Loaded 14505 records for participant 38\n",
      "2025-10-02 12:52:17,919 - INFO - Loaded 14505 records for participant 38\n",
      "2025-10-02 12:52:17,934 - INFO - Chunk 7 combined: 76560 records\n",
      "2025-10-02 12:52:17,981 - INFO - Processing chunk 8/9 (files 36-40)\n",
      "2025-10-02 12:52:17,934 - INFO - Chunk 7 combined: 76560 records\n",
      "2025-10-02 12:52:17,981 - INFO - Processing chunk 8/9 (files 36-40)\n",
      "2025-10-02 12:52:18,059 - INFO - Loaded 17205 records for participant 39\n",
      "2025-10-02 12:52:18,059 - INFO - Loaded 17205 records for participant 39\n",
      "2025-10-02 12:52:18,122 - INFO - Loaded 14310 records for participant 41\n",
      "2025-10-02 12:52:18,122 - INFO - Loaded 14310 records for participant 41\n",
      "2025-10-02 12:52:18,184 - INFO - Loaded 14520 records for participant 42\n",
      "2025-10-02 12:52:18,184 - INFO - Loaded 14520 records for participant 42\n",
      "2025-10-02 12:52:18,247 - INFO - Loaded 14520 records for participant 43\n",
      "2025-10-02 12:52:18,247 - INFO - Loaded 14520 records for participant 43\n",
      "2025-10-02 12:52:18,309 - INFO - Loaded 14535 records for participant 44\n",
      "2025-10-02 12:52:18,325 - INFO - Chunk 8 combined: 75090 records\n",
      "2025-10-02 12:52:18,309 - INFO - Loaded 14535 records for participant 44\n",
      "2025-10-02 12:52:18,325 - INFO - Chunk 8 combined: 75090 records\n",
      "2025-10-02 12:52:18,372 - INFO - Processing chunk 9/9 (files 41-45)\n",
      "2025-10-02 12:52:18,372 - INFO - Processing chunk 9/9 (files 41-45)\n",
      "2025-10-02 12:52:18,434 - INFO - Loaded 15570 records for participant 45\n",
      "2025-10-02 12:52:18,434 - INFO - Loaded 15570 records for participant 45\n",
      "2025-10-02 12:52:18,512 - INFO - Loaded 14655 records for participant 46\n",
      "2025-10-02 12:52:18,512 - INFO - Loaded 14655 records for participant 46\n",
      "2025-10-02 12:52:18,575 - INFO - Loaded 15690 records for participant 47\n",
      "2025-10-02 12:52:18,575 - INFO - Loaded 15690 records for participant 47\n",
      "2025-10-02 12:52:18,653 - INFO - Loaded 17340 records for participant 48\n",
      "2025-10-02 12:52:18,653 - INFO - Loaded 17340 records for participant 48\n",
      "2025-10-02 12:52:18,716 - INFO - Loaded 15315 records for participant 49\n",
      "2025-10-02 12:52:18,716 - INFO - Loaded 15315 records for participant 49\n",
      "2025-10-02 12:52:18,731 - INFO - Chunk 9 combined: 78570 records\n",
      "2025-10-02 12:52:18,794 - INFO - Combining all chunks into final dataset...\n",
      "2025-10-02 12:52:18,731 - INFO - Chunk 9 combined: 78570 records\n",
      "2025-10-02 12:52:18,794 - INFO - Combining all chunks into final dataset...\n",
      "2025-10-02 12:52:19,231 - INFO - Successfully loaded complete CGMacros data: 687580 total records from 45 participants\n",
      "2025-10-02 12:52:19,231 - INFO - Successfully loaded complete CGMacros data: 687580 total records from 45 participants\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after CGMacros loading: 174.4 MB\n",
      "CGMacros data shape: (687580, 21)\n",
      "CGMacros data memory usage: 53.5 MB\n",
      "\n",
      "CGMacros data overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 687580 entries, 0 to 687579\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   Unnamed: 0           137785 non-null  float32       \n",
      " 1   Timestamp            687580 non-null  datetime64[ns]\n",
      " 2   Libre GL             687360 non-null  float32       \n",
      " 3   Dexcom GL            629825 non-null  float32       \n",
      " 4   HR                   610256 non-null  float32       \n",
      " 5   Calories (Activity)  652134 non-null  float32       \n",
      " 6   METs                 501078 non-null  float32       \n",
      " 7   Meal Type            1706 non-null    category      \n",
      " 8   Calories             1706 non-null    float32       \n",
      " 9   Carbs                1706 non-null    float32       \n",
      " 10  Protein              1706 non-null    float32       \n",
      " 11  Fat                  1706 non-null    float32       \n",
      " 12  Fiber                1705 non-null    float32       \n",
      " 13  Amount Consumed      43 non-null      float32       \n",
      " 14  Image path           3197 non-null    category      \n",
      " 15  participant_id       687580 non-null  int16         \n",
      " 16  Amount Consumed      1599 non-null    float32       \n",
      " 17  Steps                5655 non-null    float32       \n",
      " 18  RecordIndex          17 non-null      float32       \n",
      " 19  Intensity            156710 non-null  float32       \n",
      " 20  Sugar                39 non-null      float32       \n",
      "dtypes: category(2), datetime64[ns](1), float32(17), int16(1)\n",
      "memory usage: 53.5 MB\n",
      "None\n",
      "✅ Memory-optimized data loading complete\n"
     ]
    }
   ],
   "source": [
    "from data_loader_updated import DataLoader\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(data_dir='../data/raw')\n",
    "\n",
    "print(f\"Memory before data loading: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Load CGMacros data with chunked processing (5 files at a time)\n",
    "print(\"Loading CGMacros data with chunked processing...\")\n",
    "cgmacros_data = data_loader.load_cgmacros_data(chunk_size=5)\n",
    "\n",
    "print(f\"Memory after CGMacros loading: {get_memory_usage():.1f} MB\")\n",
    "print(f\"CGMacros data shape: {cgmacros_data.shape}\")\n",
    "print(f\"CGMacros data memory usage: {cgmacros_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Display data info\n",
    "print(\"\\nCGMacros data overview:\")\n",
    "print(cgmacros_data.info(memory_usage='deep'))\n",
    "\n",
    "print(\"✅ Memory-optimized data loading complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d30480",
   "metadata": {},
   "source": [
    "## Phase 1.5: Data Merging with Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a12b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:52:35,498 - INFO - Merging data sources with memory optimization...\n",
      "2025-10-02 12:52:35,560 - INFO - Initial CGMacros data memory usage: 53.5 MB\n",
      "2025-10-02 12:52:35,560 - INFO - Initial CGMacros data memory usage: 53.5 MB\n",
      "2025-10-02 12:52:35,601 - INFO - Loaded demographics for 45 participants\n",
      "2025-10-02 12:52:35,601 - INFO - Loaded demographics for 45 participants\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before merging: 176.1 MB\n",
      "Merging with supplementary data (demographics, microbiome, gut health)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:52:35,958 - INFO - Merged demographics data\n",
      "2025-10-02 12:52:36,270 - INFO - Reducing microbiome features from 1979 to 500 most prevalent\n",
      "2025-10-02 12:52:36,270 - INFO - Reducing microbiome features from 1979 to 500 most prevalent\n",
      "2025-10-02 12:52:36,967 - INFO - Loaded microbiome data for 45 participants with 500 microbial features\n",
      "2025-10-02 12:52:36,967 - INFO - Loaded microbiome data for 45 participants with 500 microbial features\n",
      "2025-10-02 12:52:46,843 - INFO - Merged microbiome data\n",
      "2025-10-02 12:52:46,843 - INFO - Merged microbiome data\n",
      "2025-10-02 12:52:47,218 - INFO - Loaded gut health data for 47 participants with 22 health metrics\n",
      "2025-10-02 12:52:47,218 - INFO - Loaded gut health data for 47 participants with 22 health metrics\n",
      "2025-10-02 12:52:55,196 - INFO - Merged gut health data\n",
      "2025-10-02 12:52:55,196 - INFO - Merged gut health data\n",
      "2025-10-02 12:53:03,570 - INFO - Final merged dataset: 687580 rows, 566 columns\n",
      "2025-10-02 12:53:03,570 - INFO - Final merged dataset: 687580 rows, 566 columns\n",
      "2025-10-02 12:53:03,586 - INFO - Final memory usage: 1455.5 MB\n",
      "2025-10-02 12:53:03,586 - INFO - Final memory usage: 1455.5 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after merging: 1466.8 MB\n",
      "Merged data shape: (687580, 566)\n",
      "Merged data memory usage: 1455.5 MB\n",
      "Memory after cleanup: 1527.8 MB\n",
      "\n",
      "Merged data columns:\n",
      "Total columns: 566\n",
      "Sample columns: ['Unnamed: 0', 'Timestamp', 'Libre GL', 'Dexcom GL', 'HR', 'Calories (Activity)', 'METs', 'Meal Type', 'Calories', 'Carbs'] ...\n",
      "✅ Memory-optimized data merging complete\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory before merging: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Merge with supplementary data using memory-optimized approach\n",
    "print(\"Merging with supplementary data (demographics, microbiome, gut health)...\")\n",
    "merged_data = data_loader.merge_data_sources(cgmacros_data)\n",
    "\n",
    "print(f\"Memory after merging: {get_memory_usage():.1f} MB\")\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(f\"Merged data memory usage: {merged_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Clear original cgmacros_data to free memory\n",
    "del cgmacros_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Display merged data overview\n",
    "print(\"\\nMerged data columns:\")\n",
    "print(f\"Total columns: {len(merged_data.columns)}\")\n",
    "print(\"Sample columns:\", list(merged_data.columns[:10]), \"...\")\n",
    "\n",
    "print(\"✅ Memory-optimized data merging complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd4e07",
   "metadata": {},
   "source": [
    "## Phase 2: Memory-Optimized Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59681f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before feature engineering: 1585.5 MB\n",
      "Fixing categorical columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:53:27,269 - INFO - Starting feature engineering with memory optimization...\n",
      "2025-10-02 12:53:27,312 - INFO - Initial memory usage: 1455.5 MB\n",
      "2025-10-02 12:53:27,312 - INFO - Adding temporal features...\n",
      "2025-10-02 12:53:27,312 - INFO - Initial memory usage: 1455.5 MB\n",
      "2025-10-02 12:53:27,312 - INFO - Adding temporal features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying memory-optimized feature engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:53:30,404 - INFO - Added temporal features\n",
      "2025-10-02 12:53:33,305 - INFO - Adding glucose features...\n",
      "2025-10-02 12:53:33,305 - INFO - Adding glucose features...\n",
      "2025-10-02 12:53:57,463 - INFO - Added glucose features for columns: ['Libre GL', 'Dexcom GL']\n",
      "2025-10-02 12:53:57,463 - INFO - Added glucose features for columns: ['Libre GL', 'Dexcom GL']\n",
      "2025-10-02 12:54:03,428 - INFO - Adding activity features...\n",
      "2025-10-02 12:54:03,428 - INFO - Adding activity features...\n",
      "2025-10-02 12:54:09,542 - INFO - Added activity features for columns: ['HR', 'METs', 'Calories']\n",
      "2025-10-02 12:54:09,542 - INFO - Added activity features for columns: ['HR', 'METs', 'Calories']\n",
      "2025-10-02 12:54:16,808 - INFO - Adding meal features...\n",
      "2025-10-02 12:54:16,808 - INFO - Adding meal features...\n",
      "2025-10-02 12:54:18,928 - INFO - Added meal features\n",
      "2025-10-02 12:54:18,928 - INFO - Added meal features\n",
      "2025-10-02 12:54:25,881 - INFO - Adding demographic features...\n",
      "2025-10-02 12:54:25,881 - INFO - Adding demographic features...\n",
      "2025-10-02 12:54:28,509 - INFO - Added demographic features\n",
      "2025-10-02 12:54:28,509 - INFO - Added demographic features\n",
      "2025-10-02 12:54:30,482 - INFO - Adding microbiome features...\n",
      "2025-10-02 12:54:30,482 - INFO - Adding microbiome features...\n"
     ]
    }
   ],
   "source": [
    "from feature_engineering_updated import FeatureEngineer\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "print(f\"Memory before feature engineering: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Reload the module to get the latest changes\n",
    "if 'feature_engineering_updated' in sys.modules:\n",
    "    importlib.reload(sys.modules['feature_engineering_updated'])\n",
    "    from feature_engineering_updated import FeatureEngineer\n",
    "\n",
    "# Fix all categorical columns that might cause issues\n",
    "def fix_categorical_columns(df):\n",
    "    \"\"\"Fix categorical columns by adding common missing value categories\"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    \n",
    "    categorical_fixes = {\n",
    "        'Meal Type': 'No Meal',\n",
    "        'Gender': 'Unknown',\n",
    "        'Image path': 'No Image'\n",
    "    }\n",
    "    \n",
    "    for col, fill_value in categorical_fixes.items():\n",
    "        if col in df_fixed.columns and df_fixed[col].dtype.name == 'category':\n",
    "            # Add category if not already present\n",
    "            if fill_value not in df_fixed[col].cat.categories:\n",
    "                df_fixed[col] = df_fixed[col].cat.add_categories([fill_value])\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "print(\"Fixing categorical columns...\")\n",
    "merged_data_fixed = fix_categorical_columns(merged_data)\n",
    "\n",
    "# Initialize feature engineer with memory optimization\n",
    "feature_engineer = FeatureEngineer(memory_efficient=True)\n",
    "\n",
    "# Apply feature engineering with memory monitoring\n",
    "print(\"Applying memory-optimized feature engineering...\")\n",
    "featured_data = feature_engineer.engineer_features(merged_data_fixed)\n",
    "\n",
    "print(f\"Memory after feature engineering: {get_memory_usage():.1f} MB\")\n",
    "print(f\"Featured data shape: {featured_data.shape}\")\n",
    "print(f\"Featured data memory usage: {featured_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Clear merged_data to free memory\n",
    "del merged_data, merged_data_fixed\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Show feature engineering results\n",
    "print(\"\\nFeature engineering summary:\")\n",
    "print(f\"Total features: {len(featured_data.columns)}\")\n",
    "new_features = [col for col in featured_data.columns if any(x in col for x in ['_mean', '_std', '_div', '_richness', '_zone'])]\n",
    "print(f\"Engineered features: {len(new_features)}\")\n",
    "print(\"Sample engineered features:\", new_features[:10])\n",
    "\n",
    "print(\"✅ Memory-optimized feature engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc393d3",
   "metadata": {},
   "source": [
    "## Phase 3: Target Variable Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e77192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_updated import compute_ccr, remove_nutrient_columns, validate_ccr\n",
    "\n",
    "print(f\"Memory before target computation: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Compute CCR target variable\n",
    "print(\"Computing CCR (Carbohydrate Caloric Ratio) target variable...\")\n",
    "target_data = compute_ccr(featured_data)\n",
    "\n",
    "# Validate CCR computation\n",
    "is_valid, validation_msg = validate_ccr(target_data)\n",
    "print(f\"CCR validation: {validation_msg}\")\n",
    "\n",
    "if is_valid:\n",
    "    # Remove nutrient columns to prevent data leakage\n",
    "    target_data = remove_nutrient_columns(target_data)\n",
    "    \n",
    "    print(f\"Memory after target computation: {get_memory_usage():.1f} MB\")\n",
    "    print(f\"Target data shape: {target_data.shape}\")\n",
    "    \n",
    "    # Clear featured_data to free memory\n",
    "    del featured_data\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
    "    \n",
    "    # Display CCR statistics\n",
    "    ccr_stats = target_data['CCR'].describe()\n",
    "    print(\"\\nCCR target variable statistics:\")\n",
    "    print(ccr_stats)\n",
    "    \n",
    "    # Check for meal records\n",
    "    meal_records = target_data[target_data['CCR'] > 0]\n",
    "    print(f\"\\nMeal records with valid CCR: {len(meal_records)} out of {len(target_data)}\")\n",
    "    print(f\"Percentage of meal records: {len(meal_records)/len(target_data)*100:.1f}%\")\n",
    "    \n",
    "    print(\"✅ Target variable computation complete\")\n",
    "else:\n",
    "    print(\"❌ CCR validation failed. Cannot proceed with modeling.\")\n",
    "    raise ValueError(validation_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41724ed9",
   "metadata": {},
   "source": [
    "## Phase 4: Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Memory before data preparation: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Prepare modeling dataset - use ALL meal records for training\n",
    "modeling_data = target_data[target_data['CCR'] > 0].copy()\n",
    "print(f\"Using {len(modeling_data)} meal records for modeling (100% of available data)\")\n",
    "\n",
    "# Separate features and target\n",
    "feature_columns = [col for col in modeling_data.columns if col not in ['CCR', 'participant_id', 'Timestamp']]\n",
    "X = modeling_data[feature_columns]\n",
    "y = modeling_data['CCR']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "X_filled = X.fillna(0)  # Simple imputation for now\n",
    "missing_counts = X.isnull().sum()\n",
    "cols_with_missing = missing_counts[missing_counts > 0]\n",
    "print(f\"Columns with missing values: {len(cols_with_missing)}\")\n",
    "\n",
    "# Split data - use larger training set since we have full dataset now\n",
    "print(\"Splitting data into train/test sets (80/20)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filled, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"Memory after data preparation: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Clear unnecessary data\n",
    "del target_data, modeling_data, X, y, X_filled\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
    "print(\"✅ Data preparation for modeling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39683484",
   "metadata": {},
   "source": [
    "## Phase 5: Baseline Model Training with Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "print(f\"Memory before model training: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Train R²: {train_r2:.4f}\")\n",
    "    print(f\"  Test R²:  {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Test MAE:  {test_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nMemory after model training: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL RESULTS WITH FULL DATASET\")\n",
    "print(\"=\"*60)\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Best model\n",
    "best_model = max(results.keys(), key=lambda x: results[x]['test_r2'])\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "print(f\"Best test R²: {results[best_model]['test_r2']:.4f}\")\n",
    "\n",
    "print(\"✅ Baseline model training complete with full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc16d4",
   "metadata": {},
   "source": [
    "## Phase 6: Memory Usage Summary and Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f35983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEMORY OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Final memory usage: {get_memory_usage():.1f} MB\")\n",
    "print(f\"Available system memory: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "print(f\"Memory utilization: {get_memory_usage() / 1024:.2f} GB\")\n",
    "\n",
    "print(\"\\n✅ MEMORY OPTIMIZATION SUCCESS!\")\n",
    "print(\"   - Processed complete dataset without memory errors\")\n",
    "print(f\"   - Used all {len(X_train) + len(X_test)} meal records for modeling\")\n",
    "print(\"   - Achieved reasonable model performance with full data\")\n",
    "print(\"   - Memory usage kept under control with chunking and optimization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Advanced model training (XGBoost, LightGBM) with hyperparameter tuning\")\n",
    "print(\"2. Ensemble methods for improved performance\")\n",
    "print(\"3. Feature importance analysis\")\n",
    "print(\"4. Cross-validation for robust evaluation\")\n",
    "print(\"5. Final model selection and deployment preparation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
