{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60909a8b",
   "metadata": {},
   "source": [
    "# Memory-Optimized CGMacros CCR Prediction Pipeline\n",
    "\n",
    "This notebook executes the complete pipeline with memory optimizations to handle the full dataset (687,580 records) efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9e42b",
   "metadata": {},
   "source": [
    "## Phase 0: Environment Setup and Memory Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef54c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Memory Analysis:\n",
      "  Total RAM: 7.7 GB\n",
      "  Available RAM: 1.3 GB\n",
      "  Initial process memory: 115.9 MB\n",
      "‚ö†Ô∏è LOW MEMORY SYSTEM: Using ultra-conservative settings\n",
      "‚úÖ Environment setup complete with memory strategy: ultra_conservative\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory monitoring function\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "# Check system resources and set memory limits\n",
    "available_memory_gb = psutil.virtual_memory().available / 1024**3\n",
    "total_memory_gb = psutil.virtual_memory().total / 1024**3\n",
    "\n",
    "print(f\"System Memory Analysis:\")\n",
    "print(f\"  Total RAM: {total_memory_gb:.1f} GB\")\n",
    "print(f\"  Available RAM: {available_memory_gb:.1f} GB\")\n",
    "print(f\"  Initial process memory: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Set memory management strategy based on available RAM\n",
    "if available_memory_gb < 2.0:\n",
    "    print(\"‚ö†Ô∏è LOW MEMORY SYSTEM: Using ultra-conservative settings\")\n",
    "    MEMORY_STRATEGY = \"ultra_conservative\"\n",
    "elif available_memory_gb < 4.0:\n",
    "    print(\"üìä MODERATE MEMORY SYSTEM: Using conservative settings\")\n",
    "    MEMORY_STRATEGY = \"conservative\" \n",
    "else:\n",
    "    print(\"üöÄ HIGH MEMORY SYSTEM: Using standard settings\")\n",
    "    MEMORY_STRATEGY = \"standard\"\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete with memory strategy:\", MEMORY_STRATEGY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeaf8f3",
   "metadata": {},
   "source": [
    "## Phase 1: Memory-Optimized Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bbda5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 13:11:08,939 - INFO - Loading CGMacros participant files with memory optimization...\n",
      "2025-10-02 13:11:08,939 - INFO - Found 45 participant files. Processing in chunks of 2...\n",
      "2025-10-02 13:11:08,946 - INFO - Processing chunk 1/23 (files 1-2)\n",
      "2025-10-02 13:11:08,939 - INFO - Found 45 participant files. Processing in chunks of 2...\n",
      "2025-10-02 13:11:08,946 - INFO - Processing chunk 1/23 (files 1-2)\n",
      "2025-10-02 13:11:09,111 - INFO - Loaded 14730 records for participant 1\n",
      "2025-10-02 13:11:09,111 - INFO - Loaded 14730 records for participant 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before data loading: 116.0 MB\n",
      "Using ultra-conservative chunk size: 2 files at a time\n",
      "Loading CGMacros data with adaptive chunked processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 13:11:09,376 - INFO - Loaded 17025 records for participant 2\n",
      "2025-10-02 13:11:09,407 - INFO - Chunk 1 combined: 31755 records\n",
      "2025-10-02 13:11:09,407 - INFO - Chunk 1 combined: 31755 records\n",
      "2025-10-02 13:11:09,579 - INFO - Processing chunk 2/23 (files 3-4)\n",
      "2025-10-02 13:11:09,579 - INFO - Processing chunk 2/23 (files 3-4)\n",
      "2025-10-02 13:11:09,817 - INFO - Loaded 14565 records for participant 3\n",
      "2025-10-02 13:11:09,817 - INFO - Loaded 14565 records for participant 3\n",
      "2025-10-02 13:11:10,011 - INFO - Loaded 14275 records for participant 4\n",
      "2025-10-02 13:11:10,011 - INFO - Loaded 14275 records for participant 4\n",
      "2025-10-02 13:11:10,021 - INFO - Chunk 2 combined: 28840 records\n",
      "2025-10-02 13:11:10,102 - INFO - Processing chunk 3/23 (files 5-6)\n",
      "2025-10-02 13:11:10,021 - INFO - Chunk 2 combined: 28840 records\n",
      "2025-10-02 13:11:10,102 - INFO - Processing chunk 3/23 (files 5-6)\n",
      "2025-10-02 13:11:10,233 - INFO - Loaded 14460 records for participant 5\n",
      "2025-10-02 13:11:10,233 - INFO - Loaded 14460 records for participant 5\n",
      "2025-10-02 13:11:10,316 - INFO - Loaded 14460 records for participant 6\n",
      "2025-10-02 13:11:10,332 - INFO - Chunk 3 combined: 28920 records\n",
      "2025-10-02 13:11:10,316 - INFO - Loaded 14460 records for participant 6\n",
      "2025-10-02 13:11:10,332 - INFO - Chunk 3 combined: 28920 records\n",
      "2025-10-02 13:11:10,399 - INFO - Processing chunk 4/23 (files 7-8)\n",
      "2025-10-02 13:11:10,399 - INFO - Processing chunk 4/23 (files 7-8)\n",
      "2025-10-02 13:11:10,476 - INFO - Loaded 5655 records for participant 7\n",
      "2025-10-02 13:11:10,476 - INFO - Loaded 5655 records for participant 7\n",
      "2025-10-02 13:11:10,547 - INFO - Loaded 14760 records for participant 8\n",
      "2025-10-02 13:11:10,547 - INFO - Loaded 14760 records for participant 8\n",
      "2025-10-02 13:11:10,557 - INFO - Chunk 4 combined: 20415 records\n",
      "2025-10-02 13:11:10,616 - INFO - Processing chunk 5/23 (files 9-10)\n",
      "2025-10-02 13:11:10,557 - INFO - Chunk 4 combined: 20415 records\n",
      "2025-10-02 13:11:10,616 - INFO - Processing chunk 5/23 (files 9-10)\n",
      "2025-10-02 13:11:10,709 - INFO - Loaded 14370 records for participant 9\n",
      "2025-10-02 13:11:10,709 - INFO - Loaded 14370 records for participant 9\n",
      "2025-10-02 13:11:10,788 - INFO - Loaded 16155 records for participant 10\n",
      "2025-10-02 13:11:10,801 - INFO - Chunk 5 combined: 30525 records\n",
      "2025-10-02 13:11:10,788 - INFO - Loaded 16155 records for participant 10\n",
      "2025-10-02 13:11:10,801 - INFO - Chunk 5 combined: 30525 records\n",
      "2025-10-02 13:11:10,848 - INFO - Processing chunk 6/23 (files 11-12)\n",
      "2025-10-02 13:11:10,848 - INFO - Processing chunk 6/23 (files 11-12)\n",
      "2025-10-02 13:11:10,920 - INFO - Loaded 14805 records for participant 11\n",
      "2025-10-02 13:11:10,920 - INFO - Loaded 14805 records for participant 11\n",
      "2025-10-02 13:11:10,997 - INFO - Loaded 14655 records for participant 12\n",
      "2025-10-02 13:11:10,997 - INFO - Loaded 14655 records for participant 12\n",
      "2025-10-02 13:11:11,005 - INFO - Chunk 6 combined: 29460 records\n",
      "2025-10-02 13:11:11,065 - INFO - Processing chunk 7/23 (files 13-14)\n",
      "2025-10-02 13:11:11,005 - INFO - Chunk 6 combined: 29460 records\n",
      "2025-10-02 13:11:11,065 - INFO - Processing chunk 7/23 (files 13-14)\n",
      "2025-10-02 13:11:11,140 - INFO - Loaded 15840 records for participant 13\n",
      "2025-10-02 13:11:11,140 - INFO - Loaded 15840 records for participant 13\n",
      "2025-10-02 13:11:11,219 - INFO - Loaded 17250 records for participant 14\n",
      "2025-10-02 13:11:11,219 - INFO - Loaded 17250 records for participant 14\n",
      "2025-10-02 13:11:11,223 - INFO - Chunk 7 combined: 33090 records\n",
      "2025-10-02 13:11:11,283 - INFO - Processing chunk 8/23 (files 15-16)\n",
      "2025-10-02 13:11:11,223 - INFO - Chunk 7 combined: 33090 records\n",
      "2025-10-02 13:11:11,283 - INFO - Processing chunk 8/23 (files 15-16)\n",
      "2025-10-02 13:11:11,395 - INFO - Loaded 16875 records for participant 15\n",
      "2025-10-02 13:11:11,395 - INFO - Loaded 16875 records for participant 15\n",
      "2025-10-02 13:11:11,482 - INFO - Loaded 16290 records for participant 16\n",
      "2025-10-02 13:11:11,482 - INFO - Loaded 16290 records for participant 16\n",
      "2025-10-02 13:11:11,491 - INFO - Chunk 8 combined: 33165 records\n",
      "2025-10-02 13:11:11,550 - INFO - Processing chunk 9/23 (files 17-18)\n",
      "2025-10-02 13:11:11,491 - INFO - Chunk 8 combined: 33165 records\n",
      "2025-10-02 13:11:11,550 - INFO - Processing chunk 9/23 (files 17-18)\n",
      "2025-10-02 13:11:11,652 - INFO - Loaded 16185 records for participant 17\n",
      "2025-10-02 13:11:11,652 - INFO - Loaded 16185 records for participant 17\n",
      "2025-10-02 13:11:11,715 - INFO - Loaded 14085 records for participant 18\n",
      "2025-10-02 13:11:11,715 - INFO - Chunk 9 combined: 30270 records\n",
      "2025-10-02 13:11:11,715 - INFO - Loaded 14085 records for participant 18\n",
      "2025-10-02 13:11:11,715 - INFO - Chunk 9 combined: 30270 records\n",
      "2025-10-02 13:11:11,787 - INFO - Processing chunk 10/23 (files 19-20)\n",
      "2025-10-02 13:11:11,787 - INFO - Processing chunk 10/23 (files 19-20)\n",
      "2025-10-02 13:11:11,872 - INFO - Loaded 14430 records for participant 19\n",
      "2025-10-02 13:11:11,872 - INFO - Loaded 14430 records for participant 19\n",
      "2025-10-02 13:11:11,955 - INFO - Loaded 16260 records for participant 20\n",
      "2025-10-02 13:11:11,955 - INFO - Loaded 16260 records for participant 20\n",
      "2025-10-02 13:11:11,968 - INFO - Chunk 10 combined: 30690 records\n",
      "2025-10-02 13:11:12,024 - INFO - Processing chunk 11/23 (files 21-22)\n",
      "2025-10-02 13:11:11,968 - INFO - Chunk 10 combined: 30690 records\n",
      "2025-10-02 13:11:12,024 - INFO - Processing chunk 11/23 (files 21-22)\n",
      "2025-10-02 13:11:12,119 - INFO - Loaded 16125 records for participant 21\n",
      "2025-10-02 13:11:12,119 - INFO - Loaded 16125 records for participant 21\n",
      "2025-10-02 13:11:12,197 - INFO - Loaded 17625 records for participant 22\n",
      "2025-10-02 13:11:12,197 - INFO - Chunk 11 combined: 33750 records\n",
      "2025-10-02 13:11:12,197 - INFO - Loaded 17625 records for participant 22\n",
      "2025-10-02 13:11:12,197 - INFO - Chunk 11 combined: 33750 records\n",
      "2025-10-02 13:11:12,265 - INFO - Processing chunk 12/23 (files 23-24)\n",
      "2025-10-02 13:11:12,265 - INFO - Processing chunk 12/23 (files 23-24)\n",
      "2025-10-02 13:11:12,357 - INFO - Loaded 16185 records for participant 23\n",
      "2025-10-02 13:11:12,357 - INFO - Loaded 16185 records for participant 23\n",
      "2025-10-02 13:11:12,434 - INFO - Loaded 16320 records for participant 26\n",
      "2025-10-02 13:11:12,450 - INFO - Chunk 12 combined: 32505 records\n",
      "2025-10-02 13:11:12,434 - INFO - Loaded 16320 records for participant 26\n",
      "2025-10-02 13:11:12,450 - INFO - Chunk 12 combined: 32505 records\n",
      "2025-10-02 13:11:12,499 - INFO - Processing chunk 13/23 (files 25-26)\n",
      "2025-10-02 13:11:12,499 - INFO - Processing chunk 13/23 (files 25-26)\n",
      "2025-10-02 13:11:12,580 - INFO - Loaded 14535 records for participant 27\n",
      "2025-10-02 13:11:12,580 - INFO - Loaded 14535 records for participant 27\n",
      "2025-10-02 13:11:12,729 - INFO - Loaded 18735 records for participant 28\n",
      "2025-10-02 13:11:12,729 - INFO - Loaded 18735 records for participant 28\n",
      "2025-10-02 13:11:12,737 - INFO - Chunk 13 combined: 33270 records\n",
      "2025-10-02 13:11:12,803 - INFO - Processing chunk 14/23 (files 27-28)\n",
      "2025-10-02 13:11:12,737 - INFO - Chunk 13 combined: 33270 records\n",
      "2025-10-02 13:11:12,803 - INFO - Processing chunk 14/23 (files 27-28)\n",
      "2025-10-02 13:11:12,999 - INFO - Loaded 17730 records for participant 29\n",
      "2025-10-02 13:11:12,999 - INFO - Loaded 17730 records for participant 29\n",
      "2025-10-02 13:11:13,151 - INFO - Loaded 15465 records for participant 30\n",
      "2025-10-02 13:11:13,151 - INFO - Loaded 15465 records for participant 30\n",
      "2025-10-02 13:11:13,154 - INFO - Chunk 14 combined: 33195 records\n",
      "2025-10-02 13:11:13,215 - INFO - Processing chunk 15/23 (files 29-30)\n",
      "2025-10-02 13:11:13,154 - INFO - Chunk 14 combined: 33195 records\n",
      "2025-10-02 13:11:13,215 - INFO - Processing chunk 15/23 (files 29-30)\n",
      "2025-10-02 13:11:13,281 - INFO - Loaded 13725 records for participant 31\n",
      "2025-10-02 13:11:13,281 - INFO - Loaded 13725 records for participant 31\n",
      "2025-10-02 13:11:13,339 - INFO - Loaded 13785 records for participant 32\n",
      "2025-10-02 13:11:13,351 - INFO - Chunk 15 combined: 27510 records\n",
      "2025-10-02 13:11:13,339 - INFO - Loaded 13785 records for participant 32\n",
      "2025-10-02 13:11:13,351 - INFO - Chunk 15 combined: 27510 records\n",
      "2025-10-02 13:11:13,399 - INFO - Processing chunk 16/23 (files 31-32)\n",
      "2025-10-02 13:11:13,399 - INFO - Processing chunk 16/23 (files 31-32)\n",
      "2025-10-02 13:11:13,484 - INFO - Loaded 15840 records for participant 33\n",
      "2025-10-02 13:11:13,484 - INFO - Loaded 15840 records for participant 33\n",
      "2025-10-02 13:11:13,567 - INFO - Loaded 14700 records for participant 34\n",
      "2025-10-02 13:11:13,567 - INFO - Loaded 14700 records for participant 34\n",
      "2025-10-02 13:11:13,571 - INFO - Chunk 16 combined: 30540 records\n",
      "2025-10-02 13:11:13,623 - INFO - Processing chunk 17/23 (files 33-34)\n",
      "2025-10-02 13:11:13,571 - INFO - Chunk 16 combined: 30540 records\n",
      "2025-10-02 13:11:13,623 - INFO - Processing chunk 17/23 (files 33-34)\n",
      "2025-10-02 13:11:13,698 - INFO - Loaded 14400 records for participant 35\n",
      "2025-10-02 13:11:13,698 - INFO - Loaded 14400 records for participant 35\n",
      "2025-10-02 13:11:13,765 - INFO - Loaded 17115 records for participant 36\n",
      "2025-10-02 13:11:13,782 - INFO - Chunk 17 combined: 31515 records\n",
      "2025-10-02 13:11:13,765 - INFO - Loaded 17115 records for participant 36\n",
      "2025-10-02 13:11:13,782 - INFO - Chunk 17 combined: 31515 records\n",
      "2025-10-02 13:11:13,832 - INFO - Processing chunk 18/23 (files 35-36)\n",
      "2025-10-02 13:11:13,832 - INFO - Processing chunk 18/23 (files 35-36)\n",
      "2025-10-02 13:11:13,899 - INFO - Loaded 14505 records for participant 38\n",
      "2025-10-02 13:11:13,899 - INFO - Loaded 14505 records for participant 38\n",
      "2025-10-02 13:11:13,965 - INFO - Loaded 17205 records for participant 39\n",
      "2025-10-02 13:11:13,965 - INFO - Loaded 17205 records for participant 39\n",
      "2025-10-02 13:11:13,984 - INFO - Chunk 18 combined: 31710 records\n",
      "2025-10-02 13:11:14,032 - INFO - Processing chunk 19/23 (files 37-38)\n",
      "2025-10-02 13:11:13,984 - INFO - Chunk 18 combined: 31710 records\n",
      "2025-10-02 13:11:14,032 - INFO - Processing chunk 19/23 (files 37-38)\n",
      "2025-10-02 13:11:14,104 - INFO - Loaded 14310 records for participant 41\n",
      "2025-10-02 13:11:14,104 - INFO - Loaded 14310 records for participant 41\n",
      "2025-10-02 13:11:14,182 - INFO - Loaded 14520 records for participant 42\n",
      "2025-10-02 13:11:14,182 - INFO - Loaded 14520 records for participant 42\n",
      "2025-10-02 13:11:14,182 - INFO - Chunk 19 combined: 28830 records\n",
      "2025-10-02 13:11:14,248 - INFO - Processing chunk 20/23 (files 39-40)\n",
      "2025-10-02 13:11:14,182 - INFO - Chunk 19 combined: 28830 records\n",
      "2025-10-02 13:11:14,248 - INFO - Processing chunk 20/23 (files 39-40)\n",
      "2025-10-02 13:11:14,332 - INFO - Loaded 14520 records for participant 43\n",
      "2025-10-02 13:11:14,332 - INFO - Loaded 14520 records for participant 43\n",
      "2025-10-02 13:11:14,402 - INFO - Loaded 14535 records for participant 44\n",
      "2025-10-02 13:11:14,402 - INFO - Loaded 14535 records for participant 44\n",
      "2025-10-02 13:11:14,402 - INFO - Chunk 20 combined: 29055 records\n",
      "2025-10-02 13:11:14,480 - INFO - Processing chunk 21/23 (files 41-42)\n",
      "2025-10-02 13:11:14,402 - INFO - Chunk 20 combined: 29055 records\n",
      "2025-10-02 13:11:14,480 - INFO - Processing chunk 21/23 (files 41-42)\n",
      "2025-10-02 13:11:14,565 - INFO - Loaded 15570 records for participant 45\n",
      "2025-10-02 13:11:14,565 - INFO - Loaded 15570 records for participant 45\n",
      "2025-10-02 13:11:14,622 - INFO - Loaded 14655 records for participant 46\n",
      "2025-10-02 13:11:14,622 - INFO - Loaded 14655 records for participant 46\n",
      "2025-10-02 13:11:14,632 - INFO - Chunk 21 combined: 30225 records\n",
      "2025-10-02 13:11:14,684 - INFO - Processing chunk 22/23 (files 43-44)\n",
      "2025-10-02 13:11:14,632 - INFO - Chunk 21 combined: 30225 records\n",
      "2025-10-02 13:11:14,684 - INFO - Processing chunk 22/23 (files 43-44)\n",
      "2025-10-02 13:11:14,767 - INFO - Loaded 15690 records for participant 47\n",
      "2025-10-02 13:11:14,767 - INFO - Loaded 15690 records for participant 47\n",
      "2025-10-02 13:11:14,836 - INFO - Loaded 17340 records for participant 48\n",
      "2025-10-02 13:11:14,836 - INFO - Loaded 17340 records for participant 48\n",
      "2025-10-02 13:11:14,848 - INFO - Chunk 22 combined: 33030 records\n",
      "2025-10-02 13:11:14,898 - INFO - Processing chunk 23/23 (files 45-45)\n",
      "2025-10-02 13:11:14,848 - INFO - Chunk 22 combined: 33030 records\n",
      "2025-10-02 13:11:14,898 - INFO - Processing chunk 23/23 (files 45-45)\n",
      "2025-10-02 13:11:14,965 - INFO - Loaded 15315 records for participant 49\n",
      "2025-10-02 13:11:14,980 - INFO - Chunk 23 combined: 15315 records\n",
      "2025-10-02 13:11:14,965 - INFO - Loaded 15315 records for participant 49\n",
      "2025-10-02 13:11:14,980 - INFO - Chunk 23 combined: 15315 records\n",
      "2025-10-02 13:11:15,032 - INFO - Combining all chunks into final dataset...\n",
      "2025-10-02 13:11:15,032 - INFO - Combining all chunks into final dataset...\n",
      "2025-10-02 13:11:15,613 - INFO - Successfully loaded complete CGMacros data: 687580 total records from 45 participants\n",
      "2025-10-02 13:11:15,613 - INFO - Successfully loaded complete CGMacros data: 687580 total records from 45 participants\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after CGMacros loading: 174.6 MB\n",
      "CGMacros data shape: (687580, 21)\n",
      "CGMacros data memory usage: 53.5 MB\n",
      "\n",
      "CGMacros data loaded: 687580 records, 21 columns\n",
      "‚úÖ Memory-optimized data loading complete\n"
     ]
    }
   ],
   "source": [
    "from data_loader_updated import DataLoader\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(data_dir='../data/raw')\n",
    "\n",
    "print(f\"Memory before data loading: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Adjust chunk size based on memory strategy\n",
    "if MEMORY_STRATEGY == \"ultra_conservative\":\n",
    "    chunk_size = 2  # Very small chunks\n",
    "    print(\"Using ultra-conservative chunk size: 2 files at a time\")\n",
    "elif MEMORY_STRATEGY == \"conservative\":\n",
    "    chunk_size = 3  # Small chunks\n",
    "    print(\"Using conservative chunk size: 3 files at a time\")\n",
    "else:\n",
    "    chunk_size = 5  # Standard chunks\n",
    "    print(\"Using standard chunk size: 5 files at a time\")\n",
    "\n",
    "# Load CGMacros data with adaptive chunked processing\n",
    "print(\"Loading CGMacros data with adaptive chunked processing...\")\n",
    "cgmacros_data = data_loader.load_cgmacros_data(chunk_size=chunk_size)\n",
    "\n",
    "print(f\"Memory after CGMacros loading: {get_memory_usage():.1f} MB\")\n",
    "print(f\"CGMacros data shape: {cgmacros_data.shape}\")\n",
    "print(f\"CGMacros data memory usage: {cgmacros_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Memory safety check\n",
    "current_memory = get_memory_usage()\n",
    "if current_memory > 300:\n",
    "    print(f\"‚ö†Ô∏è WARNING: High memory usage after loading ({current_memory:.1f} MB)\")\n",
    "    print(\"Consider restarting kernel if issues persist\")\n",
    "\n",
    "# Display data info (abbreviated for low memory systems)\n",
    "if MEMORY_STRATEGY != \"ultra_conservative\":\n",
    "    print(\"\\nCGMacros data overview:\")\n",
    "    print(cgmacros_data.info(memory_usage='deep'))\n",
    "else:\n",
    "    print(f\"\\nCGMacros data loaded: {len(cgmacros_data)} records, {len(cgmacros_data.columns)} columns\")\n",
    "\n",
    "print(\"‚úÖ Memory-optimized data loading complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d30480",
   "metadata": {},
   "source": [
    "## Phase 1.5: Adaptive Data Merging with Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a12b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 13:12:42,366 - INFO - Loaded demographics for 45 participants\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before merging: 173.2 MB\n",
      "Ultra-conservative mode: Using only 20 top microbiome features\n",
      "Applying adaptive memory optimization for merging...\n",
      "‚úÖ Merged demographics data\n",
      "Memory after demographics: 278.9 MB\n",
      "Loading microbiome with 20 features...\n",
      "‚úÖ Merged demographics data\n",
      "Memory after demographics: 278.9 MB\n",
      "Loading microbiome with 20 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 13:12:42,877 - INFO - Reducing microbiome features from 1979 to 20 most prevalent\n",
      "2025-10-02 13:12:42,908 - INFO - Loaded microbiome data for 45 participants with 20 microbial features\n",
      "2025-10-02 13:12:42,908 - INFO - Loaded microbiome data for 45 participants with 20 microbial features\n",
      "2025-10-02 13:12:43,929 - INFO - Loaded gut health data for 47 participants with 22 health metrics\n",
      "2025-10-02 13:12:43,929 - INFO - Loaded gut health data for 47 participants with 22 health metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged microbiome data (20 features)\n",
      "Memory after microbiome: 335.6 MB\n",
      "‚úÖ Merged gut health data\n",
      "‚úÖ Merged gut health data\n",
      "Memory after merging: 371.2 MB\n",
      "Merged data shape: (687580, 86)\n",
      "Merged data memory usage: 196.5 MB\n",
      "Memory after cleanup: 318.7 MB\n",
      "‚úÖ Memory usage is within safe limits\n",
      "Merged data columns: 86\n",
      "‚úÖ Adaptive memory-optimized data merging complete\n",
      "Memory after merging: 371.2 MB\n",
      "Merged data shape: (687580, 86)\n",
      "Merged data memory usage: 196.5 MB\n",
      "Memory after cleanup: 318.7 MB\n",
      "‚úÖ Memory usage is within safe limits\n",
      "Merged data columns: 86\n",
      "‚úÖ Adaptive memory-optimized data merging complete\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory before merging: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Adaptive merging based on memory strategy\n",
    "if MEMORY_STRATEGY == \"ultra_conservative\":\n",
    "    max_microbiome_features = 20  # Very minimal microbiome features\n",
    "    print(\"Ultra-conservative mode: Using only 20 top microbiome features\")\n",
    "elif MEMORY_STRATEGY == \"conservative\":\n",
    "    max_microbiome_features = 50  # Reduced microbiome features\n",
    "    print(\"Conservative mode: Using 50 top microbiome features\")\n",
    "else:\n",
    "    max_microbiome_features = 100  # Reasonable number of features\n",
    "    print(\"Standard mode: Using 100 top microbiome features\")\n",
    "\n",
    "print(\"Applying adaptive memory optimization for merging...\")\n",
    "\n",
    "# Load demographics first (small dataset)\n",
    "demographics_df = data_loader.load_demographics()\n",
    "if not demographics_df.empty:\n",
    "    merged_data = cgmacros_data.merge(demographics_df, on='participant_id', how='left')\n",
    "    print(\"‚úÖ Merged demographics data\")\n",
    "    del demographics_df\n",
    "    gc.collect()\n",
    "else:\n",
    "    merged_data = cgmacros_data.copy()\n",
    "\n",
    "# Memory checkpoint\n",
    "memory_after_demographics = get_memory_usage()\n",
    "print(f\"Memory after demographics: {memory_after_demographics:.1f} MB\")\n",
    "\n",
    "# Load microbiome with adaptive feature reduction\n",
    "print(f\"Loading microbiome with {max_microbiome_features} features...\")\n",
    "microbiome_df = data_loader.load_microbiome(max_features=max_microbiome_features)\n",
    "if not microbiome_df.empty:\n",
    "    merged_data = merged_data.merge(microbiome_df, on='participant_id', how='left')\n",
    "    print(f\"‚úÖ Merged microbiome data ({max_microbiome_features} features)\")\n",
    "    del microbiome_df\n",
    "    gc.collect()\n",
    "\n",
    "# Memory checkpoint\n",
    "memory_after_microbiome = get_memory_usage()\n",
    "print(f\"Memory after microbiome: {memory_after_microbiome:.1f} MB\")\n",
    "\n",
    "# Only load gut health if memory allows\n",
    "if memory_after_microbiome < 1000:  # Less than 1GB\n",
    "    gut_health_df = data_loader.load_gut_health()\n",
    "    if not gut_health_df.empty:\n",
    "        merged_data = merged_data.merge(gut_health_df, on='participant_id', how='left')\n",
    "        print(\"‚úÖ Merged gut health data\")\n",
    "        del gut_health_df\n",
    "        gc.collect()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping gut health data due to memory constraints\")\n",
    "\n",
    "# Immediate dtype optimization\n",
    "merged_data = data_loader._optimize_dtypes(merged_data)\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory after merging: {get_memory_usage():.1f} MB\")\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(f\"Merged data memory usage: {merged_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Clear original cgmacros_data to free memory\n",
    "del cgmacros_data\n",
    "gc.collect()\n",
    "\n",
    "final_memory = get_memory_usage()\n",
    "print(f\"Memory after cleanup: {final_memory:.1f} MB\")\n",
    "\n",
    "# Safety check\n",
    "if final_memory > 1200:  # More than 1.2 GB\n",
    "    print(\"‚ùå CRITICAL: Memory usage too high! Consider restarting kernel.\")\n",
    "    print(\"Recommendation: Use ultra_conservative mode or increase system RAM\")\n",
    "elif final_memory > 800:\n",
    "    print(\"‚ö†Ô∏è WARNING: High memory usage. Monitoring closely...\")\n",
    "else:\n",
    "    print(\"‚úÖ Memory usage is within safe limits\")\n",
    "\n",
    "print(f\"Merged data columns: {len(merged_data.columns)}\")\n",
    "print(\"‚úÖ Adaptive memory-optimized data merging complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd4e07",
   "metadata": {},
   "source": [
    "## Phase 2: Memory-Optimized Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59681f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engineering_updated import FeatureEngineer\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "print(f\"Memory before feature engineering: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Check memory threshold - if too high, skip some feature engineering\n",
    "current_memory = get_memory_usage()\n",
    "if current_memory > 600:\n",
    "    print(f\"‚ö†Ô∏è High memory usage detected ({current_memory:.1f} MB). Using minimal feature engineering.\")\n",
    "    minimal_features = True\n",
    "else:\n",
    "    minimal_features = False\n",
    "\n",
    "# Reload the module to get the latest changes\n",
    "if 'feature_engineering_updated' in sys.modules:\n",
    "    importlib.reload(sys.modules['feature_engineering_updated'])\n",
    "    from feature_engineering_updated import FeatureEngineer\n",
    "\n",
    "# Fix categorical columns that might cause issues\n",
    "def fix_categorical_columns(df):\n",
    "    \"\"\"Fix categorical columns by adding common missing value categories\"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    \n",
    "    categorical_fixes = {\n",
    "        'Meal Type': 'No Meal',\n",
    "        'Gender': 'Unknown', \n",
    "        'Image path': 'No Image'\n",
    "    }\n",
    "    \n",
    "    for col, fill_value in categorical_fixes.items():\n",
    "        if col in df_fixed.columns and df_fixed[col].dtype.name == 'category':\n",
    "            # Add category if not already present\n",
    "            if fill_value not in df_fixed[col].cat.categories:\n",
    "                df_fixed[col] = df_fixed[col].cat.add_categories([fill_value])\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "print(\"Fixing categorical columns...\")\n",
    "merged_data_fixed = fix_categorical_columns(merged_data)\n",
    "\n",
    "# Initialize feature engineer with memory optimization\n",
    "feature_engineer = FeatureEngineer(memory_efficient=True)\n",
    "\n",
    "if minimal_features:\n",
    "    print(\"Applying MINIMAL feature engineering to preserve memory...\")\n",
    "    # Only add essential features\n",
    "    featured_data = merged_data_fixed.copy()\n",
    "    \n",
    "    # Add only basic glucose features (most important for CCR prediction)\n",
    "    if 'Libre GL' in featured_data.columns:\n",
    "        featured_data['glucose_mean'] = featured_data.groupby('participant_id')['Libre GL'].transform('mean')\n",
    "        featured_data['glucose_std'] = featured_data.groupby('participant_id')['Libre GL'].transform('std')\n",
    "    \n",
    "    # Add temporal features (lightweight)\n",
    "    if 'Timestamp' in featured_data.columns:\n",
    "        featured_data['hour'] = featured_data['Timestamp'].dt.hour\n",
    "        featured_data['day_of_week'] = featured_data['Timestamp'].dt.dayofweek\n",
    "    \n",
    "    print(\"Applied minimal feature engineering\")\n",
    "    \n",
    "else:\n",
    "    print(\"Applying full memory-optimized feature engineering...\")\n",
    "    featured_data = feature_engineer.engineer_features(merged_data_fixed)\n",
    "\n",
    "print(f\"Memory after feature engineering: {get_memory_usage():.1f} MB\")\n",
    "print(f\"Featured data shape: {featured_data.shape}\")\n",
    "print(f\"Featured data memory usage: {featured_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Clear merged_data to free memory\n",
    "del merged_data, merged_data_fixed\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Show feature engineering results\n",
    "print(f\"\\nTotal features: {len(featured_data.columns)}\")\n",
    "if not minimal_features:\n",
    "    new_features = [col for col in featured_data.columns if any(x in col for x in ['_mean', '_std', '_div', '_richness', '_zone'])]\n",
    "    print(f\"Engineered features: {len(new_features)}\")\n",
    "    print(\"Sample engineered features:\", new_features[:10])\n",
    "\n",
    "print(\"‚úÖ Memory-optimized feature engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc393d3",
   "metadata": {},
   "source": [
    "## Phase 3: Target Variable Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e77192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_updated import compute_ccr, remove_nutrient_columns, validate_ccr\n",
    "\n",
    "print(f\"Memory before target computation: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Compute CCR target variable\n",
    "print(\"Computing CCR (Carbohydrate Caloric Ratio) target variable...\")\n",
    "target_data = compute_ccr(featured_data)\n",
    "\n",
    "# Validate CCR computation\n",
    "is_valid, validation_msg = validate_ccr(target_data)\n",
    "print(f\"CCR validation: {validation_msg}\")\n",
    "\n",
    "if is_valid:\n",
    "    # Remove nutrient columns to prevent data leakage\n",
    "    target_data = remove_nutrient_columns(target_data)\n",
    "    \n",
    "    print(f\"Memory after target computation: {get_memory_usage():.1f} MB\")\n",
    "    print(f\"Target data shape: {target_data.shape}\")\n",
    "    \n",
    "    # Clear featured_data to free memory\n",
    "    del featured_data\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
    "    \n",
    "    # Display CCR statistics\n",
    "    ccr_stats = target_data['CCR'].describe()\n",
    "    print(\"\\nCCR target variable statistics:\")\n",
    "    print(ccr_stats)\n",
    "    \n",
    "    # Check for meal records\n",
    "    meal_records = target_data[target_data['CCR'] > 0]\n",
    "    print(f\"\\nMeal records with valid CCR: {len(meal_records)} out of {len(target_data)}\")\n",
    "    print(f\"Percentage of meal records: {len(meal_records)/len(target_data)*100:.1f}%\")\n",
    "    \n",
    "    print(\"‚úÖ Target variable computation complete\")\n",
    "else:\n",
    "    print(\"‚ùå CCR validation failed. Cannot proceed with modeling.\")\n",
    "    raise ValueError(validation_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41724ed9",
   "metadata": {},
   "source": [
    "## Phase 4: Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Memory before data preparation: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Prepare modeling dataset - use ALL meal records for training\n",
    "modeling_data = target_data[target_data['CCR'] > 0].copy()\n",
    "print(f\"Using {len(modeling_data)} meal records for modeling (100% of available data)\")\n",
    "\n",
    "# Separate features and target\n",
    "feature_columns = [col for col in modeling_data.columns if col not in ['CCR', 'participant_id', 'Timestamp']]\n",
    "X = modeling_data[feature_columns]\n",
    "y = modeling_data['CCR']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "X_filled = X.fillna(0)  # Simple imputation for now\n",
    "missing_counts = X.isnull().sum()\n",
    "cols_with_missing = missing_counts[missing_counts > 0]\n",
    "print(f\"Columns with missing values: {len(cols_with_missing)}\")\n",
    "\n",
    "# Split data - use larger training set since we have full dataset now\n",
    "print(\"Splitting data into train/test sets (80/20)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filled, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"Memory after data preparation: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Clear unnecessary data\n",
    "del target_data, modeling_data, X, y, X_filled\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Memory after cleanup: {get_memory_usage():.1f} MB\")\n",
    "print(\"‚úÖ Data preparation for modeling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39683484",
   "metadata": {},
   "source": [
    "## Phase 5: Baseline Model Training with Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "print(f\"Memory before model training: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"  Test R¬≤:  {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Test MAE:  {test_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nMemory after model training: {get_memory_usage():.1f} MB\")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL RESULTS WITH FULL DATASET\")\n",
    "print(\"=\"*60)\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Best model\n",
    "best_model = max(results.keys(), key=lambda x: results[x]['test_r2'])\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "print(f\"Best test R¬≤: {results[best_model]['test_r2']:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Baseline model training complete with full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc16d4",
   "metadata": {},
   "source": [
    "## Phase 6: Memory Usage Summary and Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f35983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEMORY OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Final memory usage: {get_memory_usage():.1f} MB\")\n",
    "print(f\"Available system memory: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "print(f\"Memory utilization: {get_memory_usage() / 1024:.2f} GB\")\n",
    "\n",
    "print(\"\\n‚úÖ MEMORY OPTIMIZATION SUCCESS!\")\n",
    "print(\"   - Processed complete dataset without memory errors\")\n",
    "print(f\"   - Used all {len(X_train) + len(X_test)} meal records for modeling\")\n",
    "print(\"   - Achieved reasonable model performance with full data\")\n",
    "print(\"   - Memory usage kept under control with chunking and optimization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Advanced model training (XGBoost, LightGBM) with hyperparameter tuning\")\n",
    "print(\"2. Ensemble methods for improved performance\")\n",
    "print(\"3. Feature importance analysis\")\n",
    "print(\"4. Cross-validation for robust evaluation\")\n",
    "print(\"5. Final model selection and deployment preparation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
