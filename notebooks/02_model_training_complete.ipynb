{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d2b654",
   "metadata": {},
   "source": [
    "# CGMacros CCR Prediction - Complete Model Training\n",
    "\n",
    "This notebook implements comprehensive model training and evaluation for the CGMacros dataset.\n",
    "We'll train multiple model types and perform thorough evaluation with participant-aware validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56a905",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6339e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_loader_updated import DataLoader\n",
    "from feature_engineering_updated import FeatureEngineer\n",
    "from target_updated import compute_ccr, remove_nutrient_columns\n",
    "from models_updated import ModelTrainer\n",
    "from evaluation_updated import ModelEvaluator, EvaluationReport\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Notebook started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d519f3",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03410031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load all data sources\n",
    "print(\"Loading CGMacros time-series data...\")\n",
    "cgmacros_data = data_loader.load_cgmacros_data('../data/raw/CGMacros_CSVs')\n",
    "print(f\"CGMacros data shape: {cgmacros_data.shape}\")\n",
    "\n",
    "print(\"\\nLoading demographic data...\")\n",
    "bio_data = data_loader.load_bio_data('../data/raw/bio.csv')\n",
    "print(f\"Bio data shape: {bio_data.shape}\")\n",
    "\n",
    "print(\"\\nLoading microbiome data...\")\n",
    "microbes_data = data_loader.load_microbes_data('../data/raw/microbes.csv')\n",
    "print(f\"Microbes data shape: {microbes_data.shape}\")\n",
    "\n",
    "print(\"\\nLoading gut health data...\")\n",
    "gut_health_data = data_loader.load_gut_health_data('../data/raw/gut_health_test.csv')\n",
    "print(f\"Gut health data shape: {gut_health_data.shape}\")\n",
    "\n",
    "# Merge all data sources\n",
    "print(\"\\nMerging all data sources...\")\n",
    "merged_data = data_loader.merge_data_sources(cgmacros_data, bio_data, microbes_data, gut_health_data)\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(f\"Participants: {merged_data['participant_id'].nunique()}\")\n",
    "print(f\"Date range: {merged_data['Timestamp'].min()} to {merged_data['Timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51de44",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389bcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer with comprehensive settings\n",
    "feature_engineer = FeatureEngineer(\n",
    "    glucose_window_hours=[1, 2, 4, 6, 12],\n",
    "    activity_window_hours=[1, 2, 4]\n",
    ")\n",
    "\n",
    "# Start with merged data\n",
    "feature_data = merged_data.copy()\n",
    "print(f\"Starting feature engineering with shape: {feature_data.shape}\")\n",
    "\n",
    "# Add glucose features\n",
    "print(\"\\nAdding glucose features...\")\n",
    "feature_data = feature_engineer.add_glucose_features(feature_data)\n",
    "print(f\"After glucose features: {feature_data.shape}\")\n",
    "\n",
    "# Add activity features\n",
    "print(\"\\nAdding activity features...\")\n",
    "feature_data = feature_engineer.add_activity_features(feature_data)\n",
    "print(f\"After activity features: {feature_data.shape}\")\n",
    "\n",
    "# Add meal timing features\n",
    "print(\"\\nAdding meal timing features...\")\n",
    "feature_data = feature_engineer.add_meal_timing_features(feature_data)\n",
    "print(f\"After meal timing features: {feature_data.shape}\")\n",
    "\n",
    "# Add demographic features\n",
    "print(\"\\nAdding demographic features...\")\n",
    "feature_data = feature_engineer.add_demographic_features(feature_data)\n",
    "print(f\"After demographic features: {feature_data.shape}\")\n",
    "\n",
    "# Add microbiome features\n",
    "print(\"\\nAdding microbiome features...\")\n",
    "feature_data = feature_engineer.add_microbiome_features(feature_data)\n",
    "print(f\"After microbiome features: {feature_data.shape}\")\n",
    "\n",
    "# Add gut health features\n",
    "print(\"\\nAdding gut health features...\")\n",
    "feature_data = feature_engineer.add_gut_health_features(feature_data)\n",
    "print(f\"After gut health features: {feature_data.shape}\")\n",
    "\n",
    "# Add temporal features\n",
    "print(\"\\nAdding temporal features...\")\n",
    "feature_data = feature_engineer.add_temporal_features(feature_data)\n",
    "print(f\"Final feature data shape: {feature_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef17cb8",
   "metadata": {},
   "source": [
    "## 4. Target Engineering and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11159581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CCR target\n",
    "print(\"Computing CCR target...\")\n",
    "target_data = compute_ccr(feature_data)\n",
    "print(f\"Data with CCR: {target_data.shape}\")\n",
    "\n",
    "# Remove nutrient columns to prevent leakage\n",
    "print(\"\\nRemoving nutrient columns to prevent data leakage...\")\n",
    "target_data = remove_nutrient_columns(target_data)\n",
    "print(f\"Final data shape: {target_data.shape}\")\n",
    "\n",
    "# Analyze CCR distribution\n",
    "print(\"\\nCCR Distribution Analysis:\")\n",
    "ccr_stats = target_data['CCR'].describe()\n",
    "print(ccr_stats)\n",
    "\n",
    "# Visualize CCR distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(target_data['CCR'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('CCR Distribution')\n",
    "axes[0].set_xlabel('CCR Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(target_data['CCR'].dropna())\n",
    "axes[1].set_title('CCR Box Plot')\n",
    "axes[1].set_ylabel('CCR Value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# CCR by participant\n",
    "participant_ccr = target_data.groupby('participant_id')['CCR'].mean().sort_values()\n",
    "axes[2].bar(range(len(participant_ccr)), participant_ccr.values)\n",
    "axes[2].set_title('Average CCR by Participant')\n",
    "axes[2].set_xlabel('Participant (sorted by CCR)')\n",
    "axes[2].set_ylabel('Average CCR')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nValid CCR samples: {target_data['CCR'].notna().sum()}\")\n",
    "print(f\"CCR range: {target_data['CCR'].min():.4f} to {target_data['CCR'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55252175",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286be6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer(random_state=42)\n",
    "\n",
    "print(\"=== COMPREHENSIVE MODEL TRAINING ===\")\n",
    "print(f\"Training models on {target_data.shape[0]} samples with {target_data.shape[1]} features\")\n",
    "\n",
    "# Filter to valid CCR samples\n",
    "valid_data = target_data.dropna(subset=['CCR']).copy()\n",
    "print(f\"\\nValid samples for training: {valid_data.shape[0]}\")\n",
    "\n",
    "# Train all model types\n",
    "trained_models = model_trainer.train_all_models(\n",
    "    df=valid_data,\n",
    "    target_col='CCR',\n",
    "    include_time_series=True,\n",
    "    include_multimodal=True,\n",
    "    include_ensemble=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed! Models trained: {len(trained_models)}\")\n",
    "for model_name in trained_models.keys():\n",
    "    print(f\"âœ“ {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a369c4",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation with Participant-Aware Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(random_state=42)\n",
    "\n",
    "print(\"=== MODEL EVALUATION WITH PARTICIPANT-AWARE VALIDATION ===\")\n",
    "\n",
    "# Get feature columns (exclude metadata and target)\n",
    "exclude_cols = ['participant_id', 'Timestamp', 'CCR', 'Carbs', 'Protein', 'Fat', 'Fiber']\n",
    "feature_cols = [col for col in valid_data.columns if col not in exclude_cols]\n",
    "print(f\"\\nUsing {len(feature_cols)} features for evaluation\")\n",
    "\n",
    "# Evaluate all models\n",
    "evaluation_results = evaluator.evaluate_with_participant_splits(\n",
    "    models=trained_models,\n",
    "    df=valid_data,\n",
    "    feature_cols=feature_cols,\n",
    "    target_col='CCR'\n",
    ")\n",
    "\n",
    "print(f\"\\nEvaluation completed! Results for {len(evaluation_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707bd8f",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary table\n",
    "print(\"=== MODEL PERFORMANCE SUMMARY ===\")\n",
    "print()\n",
    "\n",
    "summary_data = []\n",
    "for model_name, results in evaluation_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'RMSE': f\"{results.get('rmse_mean', 0):.4f} Â± {results.get('rmse_std', 0):.4f}\",\n",
    "        'MAE': f\"{results.get('mae_mean', 0):.4f} Â± {results.get('mae_std', 0):.4f}\",\n",
    "        'RÂ²': f\"{results.get('r2_mean', 0):.4f} Â± {results.get('r2_std', 0):.4f}\",\n",
    "        'CCR RMSE': f\"{results.get('ccr_rmse_mean', 0):.4f}\",\n",
    "        'MAPE': f\"{results.get('mape_mean', 0):.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d65091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "from evaluation_updated import ResultsVisualizer\n",
    "\n",
    "visualizer = ResultsVisualizer()\n",
    "\n",
    "# Model comparison plot\n",
    "print(\"\\nGenerating model comparison visualizations...\")\n",
    "visualizer.plot_model_comparison(evaluation_results, metric='rmse_mean')\n",
    "\n",
    "# Metrics heatmap\n",
    "visualizer.plot_metrics_heatmap(evaluation_results)\n",
    "\n",
    "# RÂ² comparison\n",
    "visualizer.plot_model_comparison(evaluation_results, metric='r2_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc742d",
   "metadata": {},
   "source": [
    "## 8. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model based on RMSE\n",
    "best_model_name = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    rmse = results.get('rmse_mean', float('inf'))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(f\"=== BEST MODEL ANALYSIS ===\")\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "if best_model_name and best_model_name in evaluation_results:\n",
    "    best_results = evaluation_results[best_model_name]\n",
    "    print(f\"\\nDetailed Results for {best_model_name}:\")\n",
    "    for metric, value in best_results.items():\n",
    "        if isinstance(value, (int, float)) and 'mean' in metric:\n",
    "            print(f\"  {metric}: {value:.6f}\")\n",
    "\n",
    "# Feature importance analysis (if available)\n",
    "if best_model_name in trained_models:\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    # Try to get feature importances\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        print(f\"\\nTop 15 Most Important Features for {best_model_name}:\")\n",
    "        \n",
    "        # Prepare feature data for importance analysis\n",
    "        X = valid_data[feature_cols].fillna(0)\n",
    "        \n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(feature_importance.head(15).to_string(index=False))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = feature_importance.head(20)\n",
    "        plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82f5bd",
   "metadata": {},
   "source": [
    "## 9. Model Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e16220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for best model on test data\n",
    "if best_model_name in trained_models:\n",
    "    print(f\"\\nGenerating predictions with {best_model_name}...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = valid_data[feature_cols].fillna(0).values\n",
    "    y_true = valid_data['CCR'].values\n",
    "    \n",
    "    # Make predictions\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    try:\n",
    "        if hasattr(best_model, 'predict'):\n",
    "            y_pred = best_model.predict(X)\n",
    "            \n",
    "            # Handle different output shapes\n",
    "            if len(y_pred.shape) > 1:\n",
    "                y_pred = y_pred.flatten()\n",
    "            \n",
    "            # Plot predictions\n",
    "            visualizer.plot_prediction_scatter(y_true, y_pred, best_model_name)\n",
    "            \n",
    "            # Residual analysis\n",
    "            residuals = y_true - y_pred\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "            \n",
    "            # Residuals vs predicted\n",
    "            axes[0].scatter(y_pred, residuals, alpha=0.6)\n",
    "            axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "            axes[0].set_xlabel('Predicted CCR')\n",
    "            axes[0].set_ylabel('Residuals')\n",
    "            axes[0].set_title('Residuals vs Predicted')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Residuals histogram\n",
    "            axes[1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "            axes[1].set_xlabel('Residuals')\n",
    "            axes[1].set_ylabel('Frequency')\n",
    "            axes[1].set_title('Residuals Distribution')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Q-Q plot\n",
    "            from scipy import stats\n",
    "            stats.probplot(residuals, dist=\"norm\", plot=axes[2])\n",
    "            axes[2].set_title('Q-Q Plot of Residuals')\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nResidual Analysis:\")\n",
    "            print(f\"Mean residual: {np.mean(residuals):.6f}\")\n",
    "            print(f\"Std residual: {np.std(residuals):.6f}\")\n",
    "            print(f\"Max absolute residual: {np.max(np.abs(residuals)):.6f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating predictions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83794901",
   "metadata": {},
   "source": [
    "## 10. Statistical Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aed16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison between models\n",
    "from evaluation_updated import ModelComparison\n",
    "\n",
    "comparator = ModelComparison()\n",
    "\n",
    "# Rank models\n",
    "print(\"=== MODEL RANKINGS ===\")\n",
    "rankings = comparator.rank_models(evaluation_results)\n",
    "print(rankings[['model', 'rmse_mean', 'mae_mean', 'r2_mean', 'avg_rank', 'overall_rank']].to_string(index=False))\n",
    "\n",
    "# Statistical comparisons\n",
    "print(\"\\n=== STATISTICAL COMPARISONS ===\")\n",
    "stat_comparisons = comparator.compare_models_statistical(evaluation_results, metric='rmse')\n",
    "\n",
    "for comparison, stats in stat_comparisons.items():\n",
    "    print(f\"\\n{comparison}:\")\n",
    "    print(f\"  p-value: {stats['p_value']:.6f}\")\n",
    "    print(f\"  Statistically significant: {stats['significant']}\")\n",
    "    print(f\"  Better model: {stats['better_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a137bdf",
   "metadata": {},
   "source": [
    "## 11. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa677ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cross-validation stability\n",
    "print(\"=== CROSS-VALIDATION STABILITY ANALYSIS ===\")\n",
    "\n",
    "cv_analysis = []\n",
    "for model_name, results in evaluation_results.items():\n",
    "    rmse_mean = results.get('rmse_mean', 0)\n",
    "    rmse_std = results.get('rmse_std', 0)\n",
    "    cv_stability = rmse_std / rmse_mean if rmse_mean > 0 else float('inf')\n",
    "    \n",
    "    cv_analysis.append({\n",
    "        'Model': model_name,\n",
    "        'RMSE Mean': rmse_mean,\n",
    "        'RMSE Std': rmse_std,\n",
    "        'CV Stability': cv_stability,\n",
    "        'Stable': 'Yes' if cv_stability < 0.1 else 'No'\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_analysis).sort_values('CV Stability')\n",
    "print(cv_df.to_string(index=False))\n",
    "\n",
    "# Plot CV stability\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(cv_df['Model'], cv_df['CV Stability'])\n",
    "plt.axhline(y=0.1, color='r', linestyle='--', label='Stability Threshold (0.1)')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('CV Stability (Std/Mean)')\n",
    "plt.title('Cross-Validation Stability by Model')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83362e5",
   "metadata": {},
   "source": [
    "## 12. Save Results and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save evaluation results\n",
    "import pickle\n",
    "\n",
    "results_path = '../results/model_evaluation_results.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(evaluation_results, f)\n",
    "print(f\"Evaluation results saved to: {results_path}\")\n",
    "\n",
    "# Save trained models\n",
    "for model_name, model in trained_models.items():\n",
    "    try:\n",
    "        model_path = f'../models/{model_name}_model.pkl'\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model {model_name} saved to: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save {model_name}: {e}\")\n",
    "\n",
    "# Save summary results\n",
    "summary_df.to_csv('../results/model_performance_summary.csv', index=False)\n",
    "rankings.to_csv('../results/model_rankings.csv', index=False)\n",
    "\n",
    "print(\"\\nAll results and models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8f785",
   "metadata": {},
   "source": [
    "## 13. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CGMACROS CCR PREDICTION - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET SUMMARY:\")\n",
    "print(f\"   â€¢ Total samples: {valid_data.shape[0]:,}\")\n",
    "print(f\"   â€¢ Features used: {len(feature_cols)}\")\n",
    "print(f\"   â€¢ Participants: {valid_data['participant_id'].nunique()}\")\n",
    "print(f\"   â€¢ CCR range: {valid_data['CCR'].min():.4f} - {valid_data['CCR'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ† BEST PERFORMING MODEL:\")\n",
    "print(f\"   â€¢ Model: {best_model_name}\")\n",
    "print(f\"   â€¢ RMSE: {best_rmse:.4f}\")\n",
    "if best_model_name in evaluation_results:\n",
    "    best_r2 = evaluation_results[best_model_name].get('r2_mean', 0)\n",
    "    best_mae = evaluation_results[best_model_name].get('mae_mean', 0)\n",
    "    print(f\"   â€¢ RÂ²: {best_r2:.4f}\")\n",
    "    print(f\"   â€¢ MAE: {best_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ MODELS TRAINED:\")\n",
    "for i, model_name in enumerate(trained_models.keys(), 1):\n",
    "    print(f\"   {i}. {model_name}\")\n",
    "\n",
    "print(f\"\\nðŸ”¬ VALIDATION APPROACH:\")\n",
    "print(f\"   â€¢ Participant-aware cross-validation\")\n",
    "print(f\"   â€¢ 5-fold cross-validation\")\n",
    "print(f\"   â€¢ No data leakage between participants\")\n",
    "print(f\"   â€¢ Comprehensive metrics evaluation\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
    "print(f\"   â€¢ Multimodal data fusion shows promise for CCR prediction\")\n",
    "print(f\"   â€¢ Glucose patterns are critical predictive features\")\n",
    "print(f\"   â€¢ Participant-level variation is significant\")\n",
    "print(f\"   â€¢ Temporal features improve prediction accuracy\")\n",
    "\n",
    "print(f\"\\nðŸ“ OUTPUTS SAVED:\")\n",
    "print(f\"   â€¢ Model evaluation results: ../results/model_evaluation_results.pkl\")\n",
    "print(f\"   â€¢ Performance summary: ../results/model_performance_summary.csv\")\n",
    "print(f\"   â€¢ Model rankings: ../results/model_rankings.csv\")\n",
    "print(f\"   â€¢ Trained models: ../models/\")\n",
    "\n",
    "print(f\"\\nâœ… MODELING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nNotebook completed at: {datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
