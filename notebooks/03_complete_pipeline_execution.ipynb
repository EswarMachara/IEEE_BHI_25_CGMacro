{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e59a75",
   "metadata": {},
   "source": [
    "# CGMacros CCR Prediction Pipeline - IEEE BHI 2025 Track 2\n",
    "\n",
    "## Complete End-to-End Pipeline Execution\n",
    "\n",
    "This notebook provides a comprehensive, step-by-step execution of the complete CGMacros CCR (Carbohydrate Contribution Ratio) prediction pipeline for the IEEE BHI 2025 Track 2 challenge.\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "**Objective**: Develop machine learning models to predict CCR from continuous glucose monitoring (CGM) data combined with activity, demographic, microbiome, and gut health information.\n",
    "\n",
    "**Dataset**: 44 participants with multimodal time-series data:\n",
    "- CGMacros time-series files (glucose, activity, meal timing)\n",
    "- Demographic information (bio.csv)\n",
    "- Microbiome data (microbes.csv)\n",
    "- Gut health assessment (gut_health_test.csv)\n",
    "\n",
    "**Target Variable**: CCR = net_carbs / (net_carbs + protein + fat + fiber)\n",
    "\n",
    "### Pipeline Phases\n",
    "\n",
    "1. **Phase 0**: Pre-setup and Environment Configuration\n",
    "2. **Phase 1**: Data Loading and Integration\n",
    "3. **Phase 2**: Feature Engineering (200+ features)\n",
    "4. **Phase 3**: Target Engineering and Data Preparation\n",
    "5. **Phase 4**: Baseline Model Training\n",
    "6. **Phase 5**: Advanced Model Training (LSTM, GRU, Multimodal)\n",
    "7. **Phase 6**: Ensemble Model Development\n",
    "8. **Phase 7**: Comprehensive Evaluation\n",
    "9. **Phase 8**: Results Analysis and Reporting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23730b",
   "metadata": {},
   "source": [
    "## Phase 0: Pre-setup and Environment Configuration\n",
    "\n",
    "### Objectives:\n",
    "- Set up the working environment\n",
    "- Import required libraries\n",
    "- Configure paths and settings\n",
    "- Validate data availability\n",
    "\n",
    "### Expected Outputs:\n",
    "- Verified environment setup\n",
    "- Confirmed data file availability\n",
    "- Initialized configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99edb2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\Eswar\\Desktop\\IEEE_BHI_Track2\n",
      "Python path includes: ['C:\\\\Users\\\\Eswar\\\\Desktop\\\\IEEE_BHI_Track2', 'C:\\\\Users\\\\Eswar\\\\Desktop\\\\IEEE_BHI_Track2\\\\src']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = r'C:\\Users\\Eswar\\Desktop\\IEEE_BHI_Track2'\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes: {[p for p in sys.path if 'IEEE_BHI_Track2' in p]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52dc4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Availability Check:\n",
      "✓ bio: data/raw/bio.csv - Found\n",
      "✓ microbes: data/raw/microbes.csv - Found\n",
      "✓ gut_health: data/raw/gut_health_test.csv - Found\n",
      "✓ cgmacros_dir: data/raw/CGMacros_CSVs/ - Found\n",
      "\n",
      "CGMacros files found: 45\n",
      "Sample files: ['CGMacros-001.csv', 'CGMacros-002.csv', 'CGMacros-003.csv', 'CGMacros-004.csv', 'CGMacros-005.csv']\n"
     ]
    }
   ],
   "source": [
    "# Validate data availability\n",
    "data_files = {\n",
    "    'bio': 'data/raw/bio.csv',\n",
    "    'microbes': 'data/raw/microbes.csv',\n",
    "    'gut_health': 'data/raw/gut_health_test.csv',\n",
    "    'cgmacros_dir': 'data/raw/CGMacros_CSVs/'\n",
    "}\n",
    "\n",
    "print(\"Data Availability Check:\")\n",
    "for name, path in data_files.items():\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"✓ {name}: {path} - {'Found' if exists else 'Missing'}\")\n",
    "    if not exists:\n",
    "        print(f\"  ERROR: Required file {path} is missing!\")\n",
    "\n",
    "# Check CGMacros files\n",
    "cgmacros_files = [f for f in os.listdir('data/raw/CGMacros_CSVs/') if f.endswith('.csv')]\n",
    "print(f\"\\nCGMacros files found: {len(cgmacros_files)}\")\n",
    "print(f\"Sample files: {cgmacros_files[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4afd090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Directory created/verified: results\n",
      "✓ Directory created/verified: models\n",
      "✓ Directory created/verified: data/processed\n",
      "\n",
      "✅ Phase 0 Complete: Environment setup finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "output_dirs = ['results', 'models', 'data/processed']\n",
    "for dir_path in output_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"✓ Directory created/verified: {dir_path}\")\n",
    "\n",
    "print(\"\\n✅ Phase 0 Complete: Environment setup finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03181e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 1: Data Loading and Integration\n",
    "\n",
    "### Objectives:\n",
    "- Load all CGMacros time-series files (44 participants)\n",
    "- Load auxiliary data (demographics, microbiome, gut health)\n",
    "- Perform data integration and temporal alignment\n",
    "- Create unified dataset structure\n",
    "\n",
    "### Expected Outputs:\n",
    "- Loaded and integrated dataset\n",
    "- Data quality summary\n",
    "- Participant coverage analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d441f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_loader_updated:Loading CGMacros participant files...\n",
      "INFO:src.data_loader_updated:Loaded 14730 records for participant 1\n",
      "INFO:src.data_loader_updated:Loaded 14730 records for participant 1\n",
      "INFO:src.data_loader_updated:Loaded 17025 records for participant 2\n",
      "INFO:src.data_loader_updated:Loaded 17025 records for participant 2\n",
      "INFO:src.data_loader_updated:Loaded 14565 records for participant 3\n",
      "INFO:src.data_loader_updated:Loaded 14565 records for participant 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_loader_updated:Loading CGMacros participant files...\n",
      "INFO:src.data_loader_updated:Loaded 14730 records for participant 1\n",
      "INFO:src.data_loader_updated:Loaded 14730 records for participant 1\n",
      "INFO:src.data_loader_updated:Loaded 17025 records for participant 2\n",
      "INFO:src.data_loader_updated:Loaded 17025 records for participant 2\n",
      "INFO:src.data_loader_updated:Loaded 14565 records for participant 3\n",
      "INFO:src.data_loader_updated:Loaded 14565 records for participant 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoader initialized\n",
      "\n",
      "Loading data sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_loader_updated:Loading CGMacros participant files...\n",
      "INFO:src.data_loader_updated:Loaded 14730 records for participant 1\n",
      "INFO:src.data_loader_updated:Loaded 14730 records for participant 1\n",
      "INFO:src.data_loader_updated:Loaded 17025 records for participant 2\n",
      "INFO:src.data_loader_updated:Loaded 17025 records for participant 2\n",
      "INFO:src.data_loader_updated:Loaded 14565 records for participant 3\n",
      "INFO:src.data_loader_updated:Loaded 14565 records for participant 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoader initialized\n",
      "\n",
      "Loading data sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_loader_updated:Loaded 14275 records for participant 4\n",
      "INFO:src.data_loader_updated:Loaded 14460 records for participant 5\n",
      "INFO:src.data_loader_updated:Loaded 14460 records for participant 5\n",
      "INFO:src.data_loader_updated:Loaded 14460 records for participant 6\n",
      "INFO:src.data_loader_updated:Loaded 14460 records for participant 6\n",
      "INFO:src.data_loader_updated:Loaded 5655 records for participant 7\n",
      "INFO:src.data_loader_updated:Loaded 5655 records for participant 7\n",
      "INFO:src.data_loader_updated:Loaded 14760 records for participant 8\n",
      "INFO:src.data_loader_updated:Loaded 14760 records for participant 8\n",
      "INFO:src.data_loader_updated:Loaded 14370 records for participant 9\n",
      "INFO:src.data_loader_updated:Loaded 14370 records for participant 9\n",
      "INFO:src.data_loader_updated:Loaded 16155 records for participant 10\n",
      "INFO:src.data_loader_updated:Loaded 16155 records for participant 10\n",
      "INFO:src.data_loader_updated:Loaded 14805 records for participant 11\n",
      "INFO:src.data_loader_updated:Loaded 14805 records for participant 11\n",
      "INFO:src.data_loader_updated:Loaded 14655 records for participant 12\n",
      "INFO:src.data_loader_updated:Loaded 14655 records for participant 12\n",
      "INFO:src.data_loader_updated:Loaded 15840 records for participant 13\n",
      "INFO:src.data_loader_updated:Loaded 15840 records for participant 13\n",
      "INFO:src.data_loader_updated:Loaded 17250 records for participant 14\n",
      "INFO:src.data_loader_updated:Loaded 17250 records for participant 14\n",
      "INFO:src.data_loader_updated:Loaded 16875 records for participant 15\n",
      "INFO:src.data_loader_updated:Loaded 16875 records for participant 15\n",
      "INFO:src.data_loader_updated:Loaded 16290 records for participant 16\n",
      "INFO:src.data_loader_updated:Loaded 16290 records for participant 16\n",
      "INFO:src.data_loader_updated:Loaded 16185 records for participant 17\n",
      "INFO:src.data_loader_updated:Loaded 16185 records for participant 17\n",
      "INFO:src.data_loader_updated:Loaded 14085 records for participant 18\n",
      "INFO:src.data_loader_updated:Loaded 14085 records for participant 18\n",
      "INFO:src.data_loader_updated:Loaded 14430 records for participant 19\n",
      "INFO:src.data_loader_updated:Loaded 14430 records for participant 19\n",
      "INFO:src.data_loader_updated:Loaded 16260 records for participant 20\n",
      "INFO:src.data_loader_updated:Loaded 16260 records for participant 20\n",
      "INFO:src.data_loader_updated:Loaded 16125 records for participant 21\n",
      "INFO:src.data_loader_updated:Loaded 16125 records for participant 21\n",
      "INFO:src.data_loader_updated:Loaded 17625 records for participant 22\n",
      "INFO:src.data_loader_updated:Loaded 17625 records for participant 22\n",
      "INFO:src.data_loader_updated:Loaded 16185 records for participant 23\n",
      "INFO:src.data_loader_updated:Loaded 16185 records for participant 23\n",
      "INFO:src.data_loader_updated:Loaded 16320 records for participant 26\n",
      "INFO:src.data_loader_updated:Loaded 16320 records for participant 26\n",
      "INFO:src.data_loader_updated:Loaded 14535 records for participant 27\n",
      "INFO:src.data_loader_updated:Loaded 14535 records for participant 27\n",
      "INFO:src.data_loader_updated:Loaded 18735 records for participant 28\n",
      "INFO:src.data_loader_updated:Loaded 18735 records for participant 28\n",
      "INFO:src.data_loader_updated:Loaded 17730 records for participant 29\n",
      "INFO:src.data_loader_updated:Loaded 17730 records for participant 29\n",
      "INFO:src.data_loader_updated:Loaded 15465 records for participant 30\n",
      "INFO:src.data_loader_updated:Loaded 15465 records for participant 30\n",
      "INFO:src.data_loader_updated:Loaded 13725 records for participant 31\n",
      "INFO:src.data_loader_updated:Loaded 13725 records for participant 31\n",
      "INFO:src.data_loader_updated:Loaded 13785 records for participant 32\n",
      "INFO:src.data_loader_updated:Loaded 13785 records for participant 32\n",
      "INFO:src.data_loader_updated:Loaded 15840 records for participant 33\n",
      "INFO:src.data_loader_updated:Loaded 15840 records for participant 33\n",
      "INFO:src.data_loader_updated:Loaded 14700 records for participant 34\n",
      "INFO:src.data_loader_updated:Loaded 14700 records for participant 34\n",
      "INFO:src.data_loader_updated:Loaded 14400 records for participant 35\n",
      "INFO:src.data_loader_updated:Loaded 14400 records for participant 35\n",
      "INFO:src.data_loader_updated:Loaded 17115 records for participant 36\n",
      "INFO:src.data_loader_updated:Loaded 17115 records for participant 36\n",
      "INFO:src.data_loader_updated:Loaded 14505 records for participant 38\n",
      "INFO:src.data_loader_updated:Loaded 14505 records for participant 38\n",
      "INFO:src.data_loader_updated:Loaded 17205 records for participant 39\n",
      "INFO:src.data_loader_updated:Loaded 17205 records for participant 39\n",
      "INFO:src.data_loader_updated:Loaded 14310 records for participant 41\n",
      "INFO:src.data_loader_updated:Loaded 14310 records for participant 41\n",
      "INFO:src.data_loader_updated:Loaded 14520 records for participant 42\n",
      "INFO:src.data_loader_updated:Loaded 14520 records for participant 42\n",
      "INFO:src.data_loader_updated:Loaded 14520 records for participant 43\n",
      "INFO:src.data_loader_updated:Loaded 14520 records for participant 43\n",
      "INFO:src.data_loader_updated:Loaded 14535 records for participant 44\n",
      "INFO:src.data_loader_updated:Loaded 14535 records for participant 44\n",
      "INFO:src.data_loader_updated:Loaded 15570 records for participant 45\n",
      "INFO:src.data_loader_updated:Loaded 15570 records for participant 45\n",
      "INFO:src.data_loader_updated:Loaded 14655 records for participant 46\n",
      "INFO:src.data_loader_updated:Loaded 14655 records for participant 46\n",
      "INFO:src.data_loader_updated:Loaded 15690 records for participant 47\n",
      "INFO:src.data_loader_updated:Loaded 15690 records for participant 47\n",
      "INFO:src.data_loader_updated:Loaded 17340 records for participant 48\n",
      "INFO:src.data_loader_updated:Loaded 17340 records for participant 48\n",
      "INFO:src.data_loader_updated:Loaded 15315 records for participant 49\n",
      "INFO:src.data_loader_updated:Loaded 15315 records for participant 49\n",
      "INFO:src.data_loader_updated:Combined CGMacros data: 687580 total records from 45 participants\n",
      "INFO:src.data_loader_updated:Combined CGMacros data: 687580 total records from 45 participants\n",
      "INFO:src.data_loader_updated:Loaded demographics for 45 participants\n",
      "INFO:src.data_loader_updated:Loaded demographics for 45 participants\n",
      "INFO:src.data_loader_updated:Loaded microbiome data for 45 participants with 1979 microbial features\n",
      "INFO:src.data_loader_updated:Loaded gut health data for 47 participants with 22 health metrics\n",
      "INFO:src.data_loader_updated:Loaded microbiome data for 45 participants with 1979 microbial features\n",
      "INFO:src.data_loader_updated:Loaded gut health data for 47 participants with 22 health metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_loader_updated:Loading CGMacros participant files...\n",
      "INFO:src.data_loader_updated:Loaded 14730 records for participant 1\n",
      "INFO:src.data_loader_updated:Loaded 14730 records for participant 1\n",
      "INFO:src.data_loader_updated:Loaded 17025 records for participant 2\n",
      "INFO:src.data_loader_updated:Loaded 17025 records for participant 2\n",
      "INFO:src.data_loader_updated:Loaded 14565 records for participant 3\n",
      "INFO:src.data_loader_updated:Loaded 14565 records for participant 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoader initialized\n",
      "\n",
      "Loading data sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_loader_updated:Loaded 14275 records for participant 4\n",
      "INFO:src.data_loader_updated:Loaded 14460 records for participant 5\n",
      "INFO:src.data_loader_updated:Loaded 14460 records for participant 5\n",
      "INFO:src.data_loader_updated:Loaded 14460 records for participant 6\n",
      "INFO:src.data_loader_updated:Loaded 14460 records for participant 6\n",
      "INFO:src.data_loader_updated:Loaded 5655 records for participant 7\n",
      "INFO:src.data_loader_updated:Loaded 5655 records for participant 7\n",
      "INFO:src.data_loader_updated:Loaded 14760 records for participant 8\n",
      "INFO:src.data_loader_updated:Loaded 14760 records for participant 8\n",
      "INFO:src.data_loader_updated:Loaded 14370 records for participant 9\n",
      "INFO:src.data_loader_updated:Loaded 14370 records for participant 9\n",
      "INFO:src.data_loader_updated:Loaded 16155 records for participant 10\n",
      "INFO:src.data_loader_updated:Loaded 16155 records for participant 10\n",
      "INFO:src.data_loader_updated:Loaded 14805 records for participant 11\n",
      "INFO:src.data_loader_updated:Loaded 14805 records for participant 11\n",
      "INFO:src.data_loader_updated:Loaded 14655 records for participant 12\n",
      "INFO:src.data_loader_updated:Loaded 14655 records for participant 12\n",
      "INFO:src.data_loader_updated:Loaded 15840 records for participant 13\n",
      "INFO:src.data_loader_updated:Loaded 15840 records for participant 13\n",
      "INFO:src.data_loader_updated:Loaded 17250 records for participant 14\n",
      "INFO:src.data_loader_updated:Loaded 17250 records for participant 14\n",
      "INFO:src.data_loader_updated:Loaded 16875 records for participant 15\n",
      "INFO:src.data_loader_updated:Loaded 16875 records for participant 15\n",
      "INFO:src.data_loader_updated:Loaded 16290 records for participant 16\n",
      "INFO:src.data_loader_updated:Loaded 16290 records for participant 16\n",
      "INFO:src.data_loader_updated:Loaded 16185 records for participant 17\n",
      "INFO:src.data_loader_updated:Loaded 16185 records for participant 17\n",
      "INFO:src.data_loader_updated:Loaded 14085 records for participant 18\n",
      "INFO:src.data_loader_updated:Loaded 14085 records for participant 18\n",
      "INFO:src.data_loader_updated:Loaded 14430 records for participant 19\n",
      "INFO:src.data_loader_updated:Loaded 14430 records for participant 19\n",
      "INFO:src.data_loader_updated:Loaded 16260 records for participant 20\n",
      "INFO:src.data_loader_updated:Loaded 16260 records for participant 20\n",
      "INFO:src.data_loader_updated:Loaded 16125 records for participant 21\n",
      "INFO:src.data_loader_updated:Loaded 16125 records for participant 21\n",
      "INFO:src.data_loader_updated:Loaded 17625 records for participant 22\n",
      "INFO:src.data_loader_updated:Loaded 17625 records for participant 22\n",
      "INFO:src.data_loader_updated:Loaded 16185 records for participant 23\n",
      "INFO:src.data_loader_updated:Loaded 16185 records for participant 23\n",
      "INFO:src.data_loader_updated:Loaded 16320 records for participant 26\n",
      "INFO:src.data_loader_updated:Loaded 16320 records for participant 26\n",
      "INFO:src.data_loader_updated:Loaded 14535 records for participant 27\n",
      "INFO:src.data_loader_updated:Loaded 14535 records for participant 27\n",
      "INFO:src.data_loader_updated:Loaded 18735 records for participant 28\n",
      "INFO:src.data_loader_updated:Loaded 18735 records for participant 28\n",
      "INFO:src.data_loader_updated:Loaded 17730 records for participant 29\n",
      "INFO:src.data_loader_updated:Loaded 17730 records for participant 29\n",
      "INFO:src.data_loader_updated:Loaded 15465 records for participant 30\n",
      "INFO:src.data_loader_updated:Loaded 15465 records for participant 30\n",
      "INFO:src.data_loader_updated:Loaded 13725 records for participant 31\n",
      "INFO:src.data_loader_updated:Loaded 13725 records for participant 31\n",
      "INFO:src.data_loader_updated:Loaded 13785 records for participant 32\n",
      "INFO:src.data_loader_updated:Loaded 13785 records for participant 32\n",
      "INFO:src.data_loader_updated:Loaded 15840 records for participant 33\n",
      "INFO:src.data_loader_updated:Loaded 15840 records for participant 33\n",
      "INFO:src.data_loader_updated:Loaded 14700 records for participant 34\n",
      "INFO:src.data_loader_updated:Loaded 14700 records for participant 34\n",
      "INFO:src.data_loader_updated:Loaded 14400 records for participant 35\n",
      "INFO:src.data_loader_updated:Loaded 14400 records for participant 35\n",
      "INFO:src.data_loader_updated:Loaded 17115 records for participant 36\n",
      "INFO:src.data_loader_updated:Loaded 17115 records for participant 36\n",
      "INFO:src.data_loader_updated:Loaded 14505 records for participant 38\n",
      "INFO:src.data_loader_updated:Loaded 14505 records for participant 38\n",
      "INFO:src.data_loader_updated:Loaded 17205 records for participant 39\n",
      "INFO:src.data_loader_updated:Loaded 17205 records for participant 39\n",
      "INFO:src.data_loader_updated:Loaded 14310 records for participant 41\n",
      "INFO:src.data_loader_updated:Loaded 14310 records for participant 41\n",
      "INFO:src.data_loader_updated:Loaded 14520 records for participant 42\n",
      "INFO:src.data_loader_updated:Loaded 14520 records for participant 42\n",
      "INFO:src.data_loader_updated:Loaded 14520 records for participant 43\n",
      "INFO:src.data_loader_updated:Loaded 14520 records for participant 43\n",
      "INFO:src.data_loader_updated:Loaded 14535 records for participant 44\n",
      "INFO:src.data_loader_updated:Loaded 14535 records for participant 44\n",
      "INFO:src.data_loader_updated:Loaded 15570 records for participant 45\n",
      "INFO:src.data_loader_updated:Loaded 15570 records for participant 45\n",
      "INFO:src.data_loader_updated:Loaded 14655 records for participant 46\n",
      "INFO:src.data_loader_updated:Loaded 14655 records for participant 46\n",
      "INFO:src.data_loader_updated:Loaded 15690 records for participant 47\n",
      "INFO:src.data_loader_updated:Loaded 15690 records for participant 47\n",
      "INFO:src.data_loader_updated:Loaded 17340 records for participant 48\n",
      "INFO:src.data_loader_updated:Loaded 17340 records for participant 48\n",
      "INFO:src.data_loader_updated:Loaded 15315 records for participant 49\n",
      "INFO:src.data_loader_updated:Loaded 15315 records for participant 49\n",
      "INFO:src.data_loader_updated:Combined CGMacros data: 687580 total records from 45 participants\n",
      "INFO:src.data_loader_updated:Combined CGMacros data: 687580 total records from 45 participants\n",
      "INFO:src.data_loader_updated:Loaded demographics for 45 participants\n",
      "INFO:src.data_loader_updated:Loaded demographics for 45 participants\n",
      "INFO:src.data_loader_updated:Loaded microbiome data for 45 participants with 1979 microbial features\n",
      "INFO:src.data_loader_updated:Loaded gut health data for 47 participants with 22 health metrics\n",
      "INFO:src.data_loader_updated:Loaded microbiome data for 45 participants with 1979 microbial features\n",
      "INFO:src.data_loader_updated:Loaded gut health data for 47 participants with 22 health metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CGMacros data shape: (687580, 21)\n",
      "✓ Bio data shape: (45, 24)\n",
      "✓ Microbes data shape: (45, 1980)\n",
      "✓ Gut health data shape: (47, 23)\n"
     ]
    }
   ],
   "source": [
    "# Import data loading modules\n",
    "from src.data_loader_updated import DataLoader\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader()\n",
    "print(\"✓ DataLoader initialized\")\n",
    "\n",
    "# Load all data sources\n",
    "print(\"\\nLoading data sources...\")\n",
    "cgmacros_data = data_loader.load_cgmacros_data()\n",
    "bio_data = data_loader.load_demographics()\n",
    "microbes_data = data_loader.load_microbiome()\n",
    "gut_health_data = data_loader.load_gut_health()\n",
    "\n",
    "print(f\"✓ CGMacros data shape: {cgmacros_data.shape}\")\n",
    "print(f\"✓ Bio data shape: {bio_data.shape}\")\n",
    "print(f\"✓ Microbes data shape: {microbes_data.shape}\")\n",
    "print(f\"✓ Gut health data shape: {gut_health_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13aed49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ultra-lightweight approach for memory-constrained environment...\n",
      "Creating minimal sample for pipeline development...\n",
      "Mini CGMacros sample: (6875, 21) (1.0% of original)\n",
      "Minimal bio data: (45, 6)\n",
      "Skipping microbiome and gut health data to prevent memory issues...\n",
      "Performing minimal merge...\n",
      "\n",
      "✅ Ultra-lightweight merge complete!\n",
      "✓ Final merged data shape: (6875, 26)\n",
      "✓ Unique participants: 45\n",
      "✓ Memory usage minimized for pipeline development\n",
      "\n",
      "Columns in merged data:\n",
      "✓ Total columns: 26\n",
      "✓ Column names: ['Unnamed: 0', 'Timestamp', 'Libre GL', 'Dexcom GL', 'HR', 'Calories (Activity)', 'METs', 'Meal Type', 'Calories', 'Carbs', 'Protein', 'Fat', 'Fiber', 'Amount Consumed ', 'Image path', 'participant_id', 'Amount Consumed', 'Steps', 'RecordIndex', 'Intensity', 'Sugar', 'Age', 'Gender', 'BMI', 'Body weight ', 'Height ']\n",
      "\n",
      "Sample of merged data (first 3 rows):\n",
      "   Unnamed: 0           Timestamp   Libre GL  Dexcom GL    HR  \\\n",
      "0         NaN 2024-06-11 13:14:00  84.266667      133.2  79.0   \n",
      "1       138.0 2019-11-15 22:30:00  60.200000      107.6  76.0   \n",
      "2         NaN 2020-07-01 22:10:00  76.333333        NaN  65.0   \n",
      "\n",
      "   Calories (Activity)  METs Meal Type  Calories  Carbs  ...  Amount Consumed  \\\n",
      "0               1.0884   NaN       NaN       NaN    NaN  ...              NaN   \n",
      "1               0.9312  10.0       NaN       NaN    NaN  ...              NaN   \n",
      "2               0.9978  10.0       NaN       NaN    NaN  ...              NaN   \n",
      "\n",
      "   Steps  RecordIndex  Intensity Sugar  Age  Gender        BMI  Body weight   \\\n",
      "0    NaN          NaN        0.0   NaN   24       F  27.126654         173.2   \n",
      "1    NaN          NaN        NaN   NaN   49       F  30.946742         169.2   \n",
      "2    NaN          NaN        NaN   NaN   54       F  35.811892         195.8   \n",
      "\n",
      "   Height   \n",
      "0     67.0  \n",
      "1     62.0  \n",
      "2     62.0  \n",
      "\n",
      "[3 rows x 26 columns]\n",
      "\n",
      "✅ Phase 1 Complete: Data loading and integration finished successfully!\n",
      "Note: Using minimal sample for memory-constrained environment\n"
     ]
    }
   ],
   "source": [
    "# Merge all data sources\n",
    "print(\"Merging data sources...\")\n",
    "merged_data = data_loader.merge_data_sources(cgmacros_data)\n",
    "\n",
    "print(f\"✓ Merged data shape: {merged_data.shape}\")\n",
    "print(f\"✓ Unique participants: {merged_data['participant_id'].nunique()}\")\n",
    "print(f\"✓ Date range: {merged_data['datetime'].min()} to {merged_data['datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a7582f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Assessment:\n",
      "\n",
      "Missing values per column:\n",
      "RecordIndex         6875\n",
      "Amount Consumed     6874\n",
      "Sugar               6874\n",
      "Amount Consumed     6862\n",
      "Fiber               6860\n",
      "Meal Type           6860\n",
      "Calories            6860\n",
      "Carbs               6860\n",
      "Protein             6860\n",
      "Fat                 6860\n",
      "dtype: int64\n",
      "\n",
      "Participant data coverage:\n",
      "Available columns: ['Unnamed: 0', 'Timestamp', 'Libre GL', 'Dexcom GL', 'HR', 'Calories (Activity)', 'METs', 'Meal Type', 'Calories', 'Carbs', 'Protein', 'Fat', 'Fiber', 'Amount Consumed ', 'Image path', 'participant_id', 'Amount Consumed', 'Steps', 'RecordIndex', 'Intensity', 'Sugar', 'Age', 'Gender', 'BMI', 'Body weight ', 'Height ']\n",
      "       total_records  glucose_records  meal_records\n",
      "count      45.000000        45.000000     45.000000\n",
      "mean      152.777778       139.577778      0.333333\n",
      "std        24.626195        19.161383      0.564076\n",
      "min        39.000000        39.000000      0.000000\n",
      "25%       143.000000       133.000000      0.000000\n",
      "50%       154.000000       143.000000      0.000000\n",
      "75%       164.000000       151.000000      1.000000\n",
      "max       210.000000       166.000000      2.000000\n",
      "\n",
      "Data summary:\n",
      "✓ Total records: 6875\n",
      "✓ Participants: 45\n",
      "✓ Features: 26\n",
      "\n",
      "✅ Phase 1 Complete: Data loading and integration finished successfully!\n",
      "Note: Using memory-optimized minimal sample for development\n"
     ]
    }
   ],
   "source": [
    "# Data quality assessment\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_info = merged_data.isnull().sum().sort_values(ascending=False)\n",
    "missing_info = missing_info[missing_info > 0]\n",
    "print(missing_info.head(10))\n",
    "\n",
    "print(\"\\nParticipant data coverage:\")\n",
    "participant_coverage = merged_data.groupby('participant_id').agg({\n",
    "    'datetime': 'count',\n",
    "    'glucose': lambda x: x.notna().sum(),\n",
    "    'net_carbs': lambda x: x.notna().sum()\n",
    "}).rename(columns={'datetime': 'total_records', 'glucose': 'glucose_records', 'net_carbs': 'meal_records'})\n",
    "\n",
    "print(participant_coverage.describe())\n",
    "\n",
    "print(\"\\n✅ Phase 1 Complete: Data loading and integration finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992bca88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Feature Engineering\n",
    "\n",
    "### Objectives:\n",
    "- Generate 200+ engineered features across all modalities\n",
    "- Extract glucose patterns and dynamics\n",
    "- Compute activity-based features\n",
    "- Create temporal and circadian features\n",
    "- Process microbiome and gut health features\n",
    "\n",
    "### Expected Outputs:\n",
    "- Comprehensive feature matrix\n",
    "- Feature importance analysis\n",
    "- Feature correlation assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb200a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-optimized feature engineering...\n",
      "Adding basic temporal features...\n",
      "Adding basic glucose features...\n",
      "Adding basic activity features...\n",
      "Adding basic meal features...\n",
      "Adding interaction features...\n",
      "\n",
      "✅ Memory-optimized feature engineering complete!\n",
      "✓ Original features: 26\n",
      "✓ New features created: 20\n",
      "✓ Total features: 46\n",
      "✓ Final dataset shape: (6875, 46)\n",
      "\n",
      "New features created: ['timestamp_parsed', 'hour', 'day_of_week', 'is_weekend', 'Libre GL_mean', 'Libre GL_std', 'Libre GL_rolling_mean', 'Dexcom GL_mean', 'Dexcom GL_std', 'Dexcom GL_rolling_mean']...\n"
     ]
    }
   ],
   "source": [
    "# Import feature engineering module\n",
    "from src.feature_engineering_updated import FeatureEngineer\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "print(\"✓ FeatureEngineer initialized\")\n",
    "\n",
    "# Generate comprehensive features\n",
    "print(\"\\nGenerating features (this may take a few minutes)...\")\n",
    "features_df = feature_engineer.generate_comprehensive_features(merged_data)\n",
    "\n",
    "print(f\"✓ Features generated: {features_df.shape}\")\n",
    "print(f\"✓ Total features: {features_df.shape[1] - 2}\")  # Excluding participant_id and meal_id\n",
    "print(f\"✓ Feature categories covered: glucose, activity, temporal, microbiome, gut_health\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bfa56bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Categories:\n",
      "✓ Glucose features: 0\n",
      "✓ Activity features: 2\n",
      "✓ Temporal features: 4\n",
      "✓ Microbiome features: 0\n",
      "✓ Gut health features: 0\n",
      "✓ Other features: 39\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature categories\n",
    "feature_cols = [col for col in features_df.columns if col not in ['participant_id', 'meal_id']]\n",
    "\n",
    "# Categorize features\n",
    "glucose_features = [col for col in feature_cols if 'glucose' in col.lower()]\n",
    "activity_features = [col for col in feature_cols if any(term in col.lower() for term in ['step', 'activity', 'heart'])]\n",
    "temporal_features = [col for col in feature_cols if any(term in col.lower() for term in ['hour', 'day', 'time', 'circadian'])]\n",
    "microbiome_features = [col for col in feature_cols if 'microbiome' in col.lower()]\n",
    "gut_features = [col for col in feature_cols if 'gut' in col.lower()]\n",
    "\n",
    "print(\"Feature Categories:\")\n",
    "print(f\"✓ Glucose features: {len(glucose_features)}\")\n",
    "print(f\"✓ Activity features: {len(activity_features)}\")\n",
    "print(f\"✓ Temporal features: {len(temporal_features)}\")\n",
    "print(f\"✓ Microbiome features: {len(microbiome_features)}\")\n",
    "print(f\"✓ Gut health features: {len(gut_features)}\")\n",
    "print(f\"✓ Other features: {len(feature_cols) - len(glucose_features) - len(activity_features) - len(temporal_features) - len(microbiome_features) - len(gut_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91dd884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Quality Assessment:\n",
      "\n",
      "Missing values in features:\n",
      "Features with missing values: 19\n",
      "RecordIndex         6875\n",
      "Sugar               6874\n",
      "Amount Consumed     6874\n",
      "Amount Consumed     6862\n",
      "Fiber               6860\n",
      "Meal Type           6860\n",
      "Image path          6844\n",
      "Steps               6836\n",
      "Unnamed: 0          5526\n",
      "Intensity           5277\n",
      "dtype: int64\n",
      "\n",
      "Feature statistics:\n",
      "         Unnamed: 0                      Timestamp     Libre GL    Dexcom GL  \\\n",
      "count   1349.000000                           6875  6873.000000  6281.000000   \n",
      "mean    7735.541883  2022-11-07 12:45:48.261818112   108.932548   140.860882   \n",
      "min        7.000000            2019-11-15 20:24:00    40.000000    40.000000   \n",
      "25%     4009.000000            2021-04-02 00:14:00    83.266667   113.200000   \n",
      "50%     7688.000000            2022-11-24 23:03:00   100.466667   131.400000   \n",
      "75%    11470.000000            2024-02-01 05:25:30   125.200000   156.800000   \n",
      "max    17114.000000            2025-11-10 05:07:00   404.066667   400.000000   \n",
      "std     4450.892210                            NaN    40.630582    41.217430   \n",
      "\n",
      "                HR  \n",
      "count  6080.000000  \n",
      "mean     78.880099  \n",
      "min      44.000000  \n",
      "25%      68.000000  \n",
      "50%      79.000000  \n",
      "75%      89.000000  \n",
      "max     156.000000  \n",
      "std      15.196090  \n",
      "\n",
      "✅ Phase 2 Complete: Feature engineering finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# Feature quality assessment\n",
    "print(\"Feature Quality Assessment:\")\n",
    "print(f\"\\nMissing values in features:\")\n",
    "feature_missing = features_df[feature_cols].isnull().sum().sort_values(ascending=False)\n",
    "feature_missing = feature_missing[feature_missing > 0]\n",
    "print(f\"Features with missing values: {len(feature_missing)}\")\n",
    "if len(feature_missing) > 0:\n",
    "    print(feature_missing.head(10))\n",
    "\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(features_df[feature_cols].describe().iloc[:, :5])  # Show first 5 features\n",
    "\n",
    "print(\"\\n✅ Phase 2 Complete: Feature engineering finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c5d5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Target Engineering and Data Preparation\n",
    "\n",
    "### Objectives:\n",
    "- Compute CCR (Carbohydrate Contribution Ratio) targets\n",
    "- Implement data leakage prevention\n",
    "- Create train/validation splits with participant awareness\n",
    "- Prepare final datasets for modeling\n",
    "\n",
    "### Expected Outputs:\n",
    "- CCR target variable computed\n",
    "- Participant-aware data splits\n",
    "- Clean modeling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf06901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Target engineering functions imported\n",
      "\n",
      "Preparing data for CCR computation...\n",
      "✓ Data with targets shape: (6875, 46)\n",
      "✓ Available columns for CCR computation:\n",
      "  - Carbs: 6875 non-null values\n",
      "  - Protein: 6875 non-null values\n",
      "  - Fat: 6875 non-null values\n",
      "  - Fiber: 15 non-null values\n"
     ]
    }
   ],
   "source": [
    "# Import target engineering module\n",
    "from src.target_updated import TargetEngineer\n",
    "\n",
    "# Initialize target engineer\n",
    "target_engineer = TargetEngineer()\n",
    "print(\"✓ TargetEngineer initialized\")\n",
    "\n",
    "# Merge features with original data to get target variables\n",
    "print(\"\\nMerging features with target data...\")\n",
    "data_with_targets = merged_data.merge(\n",
    "    features_df, \n",
    "    on=['participant_id'], \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Data with targets shape: {data_with_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d560bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.target_updated:Computing CCR (Carbohydrate Caloric Ratio)...\n",
      "INFO:src.target_updated:CCR computed for 15 records\n",
      "INFO:src.target_updated:CCR statistics - Mean: 0.531, Std: 0.156, Min: 0.160, Max: 0.844\n",
      "INFO:src.target_updated:CCR computed for 15 records\n",
      "INFO:src.target_updated:CCR statistics - Mean: 0.531, Std: 0.156, Min: 0.160, Max: 0.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.target_updated:Computing CCR (Carbohydrate Caloric Ratio)...\n",
      "INFO:src.target_updated:CCR computed for 15 records\n",
      "INFO:src.target_updated:CCR statistics - Mean: 0.531, Std: 0.156, Min: 0.160, Max: 0.844\n",
      "INFO:src.target_updated:CCR computed for 15 records\n",
      "INFO:src.target_updated:CCR statistics - Mean: 0.531, Std: 0.156, Min: 0.160, Max: 0.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CCR targets...\n",
      "✓ Data with CCR shape: (6875, 47)\n",
      "✓ CCR statistics:\n",
      "count    6875.000000\n",
      "mean        0.001158\n",
      "std         0.025759\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         0.843750\n",
      "Name: CCR, dtype: float64\n",
      "\n",
      "CCR distribution:\n",
      "Valid CCR values: 6875\n",
      "Missing CCR values: 0\n",
      "\n",
      "Sample CCR values:\n",
      "   Carbs  Protein  Fat  Fiber  CCR\n",
      "0    0.0      0.0  0.0    0.0  0.0\n",
      "1    0.0      0.0  0.0    0.0  0.0\n",
      "2    0.0      0.0  0.0    0.0  0.0\n",
      "3    0.0      0.0  0.0    0.0  0.0\n",
      "4    0.0      0.0  0.0    0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute CCR targets\n",
    "print(\"Computing CCR targets...\")\n",
    "data_with_ccr = target_engineer.compute_ccr_targets(data_with_targets)\n",
    "\n",
    "print(f\"✓ Data with CCR shape: {data_with_ccr.shape}\")\n",
    "print(f\"✓ CCR statistics:\")\n",
    "print(data_with_ccr['ccr'].describe())\n",
    "\n",
    "print(f\"\\nCCR distribution:\")\n",
    "print(f\"Valid CCR values: {data_with_ccr['ccr'].notna().sum()}\")\n",
    "print(f\"Missing CCR values: {data_with_ccr['ccr'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9954c729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing final modeling dataset...\n",
      "✓ Final modeling data shape: (15, 47)\n",
      "✓ Participants in final dataset: 13\n",
      "✓ Valid samples for modeling: 15\n",
      "✓ Features available for modeling: 34\n",
      "✓ Feature columns: ['Libre GL', 'Dexcom GL', 'HR', 'Calories (Activity)', 'METs', 'Calories', 'Steps', 'RecordIndex', 'Intensity', 'Sugar']...\n",
      "\n",
      "CCR statistics for modeling (non-zero values only):\n",
      "count    15.000000\n",
      "mean      0.530896\n",
      "std       0.156416\n",
      "min       0.160000\n",
      "25%       0.463158\n",
      "50%       0.500000\n",
      "75%       0.631078\n",
      "max       0.843750\n",
      "Name: CCR, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Prepare final modeling dataset\n",
    "print(\"Preparing final modeling dataset...\")\n",
    "modeling_data = target_engineer.prepare_modeling_data(data_with_ccr)\n",
    "\n",
    "print(f\"✓ Final modeling data shape: {modeling_data.shape}\")\n",
    "print(f\"✓ Participants in final dataset: {modeling_data['participant_id'].nunique()}\")\n",
    "print(f\"✓ Valid samples for modeling: {modeling_data['ccr'].notna().sum()}\")\n",
    "\n",
    "# Feature columns for modeling\n",
    "feature_columns = [col for col in modeling_data.columns \n",
    "                  if col not in ['participant_id', 'meal_id', 'ccr', 'datetime', \n",
    "                               'net_carbs', 'protein', 'fat', 'fiber']]\n",
    "\n",
    "print(f\"✓ Features available for modeling: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ab3baf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation splits...\n",
      "✓ Training data: (10, 47) (10 participants)\n",
      "✓ Validation data: (5, 47) (5 participants)\n",
      "✓ Participant overlap: 2 participants\n",
      "\n",
      "CCR distribution in splits:\n",
      "Train CCR range: 0.160 - 0.844\n",
      "Val CCR range: 0.425 - 0.571\n",
      "\n",
      "✅ Phase 3 Complete: Target engineering and data preparation finished successfully!\n",
      "Note: Working with minimal sample - only 15 meal records available\n"
     ]
    }
   ],
   "source": [
    "# Create participant-aware train/validation splits\n",
    "print(\"Creating participant-aware data splits...\")\n",
    "train_data, val_data = target_engineer.create_participant_splits(\n",
    "    modeling_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"✓ Training data: {train_data.shape} ({train_data['participant_id'].nunique()} participants)\")\n",
    "print(f\"✓ Validation data: {val_data.shape} ({val_data['participant_id'].nunique()} participants)\")\n",
    "\n",
    "# Verify no participant overlap\n",
    "train_participants = set(train_data['participant_id'].unique())\n",
    "val_participants = set(val_data['participant_id'].unique())\n",
    "overlap = train_participants.intersection(val_participants)\n",
    "print(f\"✓ Participant overlap check: {len(overlap)} (should be 0)\")\n",
    "\n",
    "print(\"\\n✅ Phase 3 Complete: Target engineering and data preparation finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2eaae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 4: Baseline Model Training\n",
    "\n",
    "### Objectives:\n",
    "- Train baseline regression models (Linear, Ridge, Lasso)\n",
    "- Train tree-based models (Random Forest, XGBoost, LightGBM)\n",
    "- Perform hyperparameter optimization\n",
    "- Evaluate baseline performance\n",
    "\n",
    "### Expected Outputs:\n",
    "- Trained baseline models\n",
    "- Performance metrics\n",
    "- Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00ec2689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up simplified baseline models for minimal dataset...\n",
      "✓ Basic ML libraries imported\n",
      "✓ Numeric feature columns: 33 out of 34\n",
      "✓ Training features: (10, 33)\n",
      "✓ Training targets: (10,)\n",
      "✓ Validation features: (5, 33)\n",
      "✓ Validation targets: (5,)\n",
      "Handling missing values...\n",
      "✓ Data prepared for modeling\n",
      "✓ Numeric feature columns: 33\n",
      "✓ Sample features: ['Libre GL', 'Dexcom GL', 'HR', 'Calories (Activity)', 'METs']\n",
      "✓ Ready for baseline model training\n"
     ]
    }
   ],
   "source": [
    "# Import modeling module\n",
    "from src.models_updated import ModelTrainer\n",
    "\n",
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer()\n",
    "print(\"✓ ModelTrainer initialized\")\n",
    "\n",
    "# Prepare data for modeling\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data['ccr']\n",
    "X_val = val_data[feature_columns]\n",
    "y_val = val_data['ccr']\n",
    "train_groups = train_data['participant_id']\n",
    "\n",
    "print(f\"✓ Training features: {X_train.shape}\")\n",
    "print(f\"✓ Training targets: {y_train.shape}\")\n",
    "print(f\"✓ Validation features: {X_val.shape}\")\n",
    "print(f\"✓ Validation targets: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48edb875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline models (adapted for small dataset)...\n",
      "Handling missing values with simple forward fill...\n",
      "✓ Missing value handling complete\n",
      "✓ Training shape: (10, 33)\n",
      "✓ Validation shape: (5, 33)\n",
      "✓ Training NaN count: 0\n",
      "✓ Validation NaN count: 0\n",
      "\n",
      "Training models on 10 samples...\n",
      "\n",
      "Training Linear Regression...\n",
      "  Train R²: 1.0000\n",
      "  Val R²: -12.8359\n",
      "  Val RMSE: 0.2092\n",
      "  Val MAE: 0.1957\n",
      "\n",
      "Training Ridge Regression...\n",
      "  Train R²: 1.0000\n",
      "  Val R²: -12.8324\n",
      "  Val RMSE: 0.2092\n",
      "  Val MAE: 0.1956\n",
      "\n",
      "Training Random Forest...\n",
      "  Train R²: 0.7836\n",
      "  Val R²: -2.1576\n",
      "  Val RMSE: 0.1000\n",
      "  Val MAE: 0.0906\n",
      "\n",
      "✅ Baseline Models Training Complete\n",
      "✓ Successfully trained: 3 models\n",
      "✓ Best model: Random Forest (R² = -2.1576)\n"
     ]
    }
   ],
   "source": [
    "# Train baseline models\n",
    "print(\"Training baseline models...\")\n",
    "baseline_results = model_trainer.train_baseline_models(\n",
    "    X_train, y_train, X_val, y_val, groups=train_groups\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Baseline Models Training Complete\")\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "for model_name, results in baseline_results.items():\n",
    "    if 'val_score' in results:\n",
    "        print(f\"{model_name}: R² = {results['val_score']:.4f}\")\n",
    "    if 'val_rmse' in results:\n",
    "        print(f\"  RMSE = {results['val_rmse']:.4f}\")\n",
    "    if 'val_mae' in results:\n",
    "        print(f\"  MAE = {results['val_mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7482c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing feature importance...\n",
      "Best baseline model: Random Forest\n",
      "Best validation R²: -2.1576\n",
      "Best validation RMSE: 0.1000\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "               feature  importance\n",
      "23             HR_mean    0.197336\n",
      "0             Libre GL    0.177111\n",
      "11                 BMI    0.176561\n",
      "4                 METs    0.145066\n",
      "31  carb_protein_ratio    0.130700\n",
      "2                   HR    0.057341\n",
      "26            METs_std    0.054934\n",
      "12        Body weight     0.034849\n",
      "1            Dexcom GL    0.023438\n",
      "13             Height     0.002664\n",
      "\n",
      "Model Performance Summary:\n",
      "Dataset size: 15 meal records from 13 participants\n",
      "Training samples: 10\n",
      "Validation samples: 5\n",
      "Features used: 33\n",
      "\n",
      "Validation Predictions vs Actual:\n",
      "      Actual  Predicted  Difference\n",
      "3930  0.5714     0.4594      0.1120\n",
      "4261  0.4248     0.4629     -0.0381\n",
      "230   0.4632     0.6226     -0.1594\n",
      "5933  0.4479     0.5081     -0.0602\n",
      "1902  0.5395     0.6226     -0.0831\n",
      "\n",
      "✅ Phase 4 Complete: Baseline model training finished successfully!\n",
      "Note: Results based on minimal sample - need full dataset for production models\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature importance for best baseline model\n",
    "best_baseline = max(baseline_results.items(), \n",
    "                   key=lambda x: x[1].get('val_score', -float('inf')))\n",
    "best_model_name, best_model_results = best_baseline\n",
    "\n",
    "print(f\"Best baseline model: {best_model_name}\")\n",
    "print(f\"Best validation R²: {best_model_results['val_score']:.4f}\")\n",
    "\n",
    "if 'feature_importance' in best_model_results:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model_results['feature_importance']\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "print(\"\\n✅ Phase 4 Complete: Baseline model training finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d8314",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 5: Advanced Model Training\n",
    "\n",
    "### Objectives:\n",
    "- Train LSTM and GRU time-series models\n",
    "- Implement multimodal fusion models\n",
    "- Develop attention-based architectures\n",
    "- Optimize deep learning hyperparameters\n",
    "\n",
    "### Expected Outputs:\n",
    "- Trained deep learning models\n",
    "- Time-series specific performance\n",
    "- Multimodal fusion results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time-series data for LSTM/GRU models\n",
    "print(\"Preparing time-series data for advanced models...\")\n",
    "\n",
    "# Create sequences for time-series models\n",
    "sequence_data = model_trainer.prepare_sequence_data(\n",
    "    train_data, val_data, feature_columns, \n",
    "    sequence_length=24, target_column='ccr'\n",
    ")\n",
    "\n",
    "X_train_seq, y_train_seq = sequence_data['train']\n",
    "X_val_seq, y_val_seq = sequence_data['val']\n",
    "\n",
    "print(f\"✓ Training sequences: {X_train_seq.shape}\")\n",
    "print(f\"✓ Validation sequences: {X_val_seq.shape}\")\n",
    "print(f\"✓ Sequence length: {X_train_seq.shape[1]}\")\n",
    "print(f\"✓ Features per timestep: {X_train_seq.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM models\n",
    "print(\"Training LSTM models...\")\n",
    "lstm_results = model_trainer.train_lstm_models(\n",
    "    X_train_seq, y_train_seq, X_val_seq, y_val_seq\n",
    ")\n",
    "\n",
    "print(\"\\n✓ LSTM Models Training Complete\")\n",
    "print(\"\\nLSTM Performance Summary:\")\n",
    "for model_name, results in lstm_results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Val Loss: {results['val_loss']:.4f}\")\n",
    "    print(f\"  Val R²: {results['val_r2']:.4f}\")\n",
    "    print(f\"  Val RMSE: {results['val_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU models\n",
    "print(\"Training GRU models...\")\n",
    "gru_results = model_trainer.train_gru_models(\n",
    "    X_train_seq, y_train_seq, X_val_seq, y_val_seq\n",
    ")\n",
    "\n",
    "print(\"\\n✓ GRU Models Training Complete\")\n",
    "print(\"\\nGRU Performance Summary:\")\n",
    "for model_name, results in gru_results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Val Loss: {results['val_loss']:.4f}\")\n",
    "    print(f\"  Val R²: {results['val_r2']:.4f}\")\n",
    "    print(f\"  Val RMSE: {results['val_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multimodal fusion models\n",
    "print(\"Training multimodal fusion models...\")\n",
    "\n",
    "# Prepare multimodal data\n",
    "multimodal_data = model_trainer.prepare_multimodal_data(\n",
    "    train_data, val_data, feature_columns\n",
    ")\n",
    "\n",
    "multimodal_results = model_trainer.train_multimodal_models(\n",
    "    multimodal_data['train'], multimodal_data['val']\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Multimodal Models Training Complete\")\n",
    "print(\"\\nMultimodal Performance Summary:\")\n",
    "for model_name, results in multimodal_results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Val Loss: {results['val_loss']:.4f}\")\n",
    "    print(f\"  Val R²: {results['val_r2']:.4f}\")\n",
    "    print(f\"  Val RMSE: {results['val_rmse']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Phase 5 Complete: Advanced model training finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024dd67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 6: Ensemble Model Development\n",
    "\n",
    "### Objectives:\n",
    "- Create ensemble models combining baseline and advanced models\n",
    "- Implement stacking and blending strategies\n",
    "- Optimize ensemble weights\n",
    "- Validate ensemble performance\n",
    "\n",
    "### Expected Outputs:\n",
    "- Optimized ensemble models\n",
    "- Ensemble performance metrics\n",
    "- Model contribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9756afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all trained models\n",
    "print(\"Preparing ensemble models...\")\n",
    "\n",
    "all_models = {\n",
    "    **baseline_results,\n",
    "    **lstm_results,\n",
    "    **gru_results,\n",
    "    **multimodal_results\n",
    "}\n",
    "\n",
    "print(f\"✓ Total models for ensemble: {len(all_models)}\")\n",
    "print(\"Models included:\")\n",
    "for model_name in all_models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fed351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from all models\n",
    "print(\"Generating predictions for ensemble...\")\n",
    "\n",
    "ensemble_predictions = model_trainer.generate_ensemble_predictions(\n",
    "    all_models, X_val, X_val_seq, multimodal_data['val']\n",
    ")\n",
    "\n",
    "print(f\"✓ Ensemble predictions shape: {ensemble_predictions.shape}\")\n",
    "print(f\"✓ Models in ensemble: {ensemble_predictions.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a94e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble models\n",
    "print(\"Training ensemble models...\")\n",
    "\n",
    "ensemble_results = model_trainer.train_ensemble_models(\n",
    "    ensemble_predictions, y_val, \n",
    "    model_names=list(all_models.keys())\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Ensemble Models Training Complete\")\n",
    "print(\"\\nEnsemble Performance Summary:\")\n",
    "for ensemble_name, results in ensemble_results.items():\n",
    "    print(f\"{ensemble_name}:\")\n",
    "    print(f\"  Val R²: {results['val_r2']:.4f}\")\n",
    "    print(f\"  Val RMSE: {results['val_rmse']:.4f}\")\n",
    "    print(f\"  Val MAE: {results['val_mae']:.4f}\")\n",
    "    \n",
    "    if 'weights' in results:\n",
    "        print(f\"  Model weights: {dict(zip(all_models.keys(), results['weights']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best overall model\n",
    "best_ensemble = max(ensemble_results.items(), \n",
    "                   key=lambda x: x[1]['val_r2'])\n",
    "best_ensemble_name, best_ensemble_results = best_ensemble\n",
    "\n",
    "print(f\"\\nBest ensemble model: {best_ensemble_name}\")\n",
    "print(f\"Best ensemble R²: {best_ensemble_results['val_r2']:.4f}\")\n",
    "print(f\"Best ensemble RMSE: {best_ensemble_results['val_rmse']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Phase 6 Complete: Ensemble model development finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9319109",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 7: Comprehensive Evaluation\n",
    "\n",
    "### Objectives:\n",
    "- Evaluate all models with comprehensive metrics\n",
    "- Perform participant-aware cross-validation\n",
    "- Conduct statistical significance testing\n",
    "- Generate detailed performance reports\n",
    "\n",
    "### Expected Outputs:\n",
    "- Complete evaluation metrics (15+ metrics)\n",
    "- Cross-validation results\n",
    "- Statistical significance tests\n",
    "- Performance comparison tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation module\n",
    "from src.evaluation_updated import ModelEvaluator, EvaluationReport\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "print(\"✓ ModelEvaluator initialized\")\n",
    "\n",
    "# Prepare all models for evaluation\n",
    "final_models = {\n",
    "    **all_models,\n",
    "    **ensemble_results\n",
    "}\n",
    "\n",
    "print(f\"✓ Total models for evaluation: {len(final_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive evaluation\n",
    "print(\"Performing comprehensive evaluation...\")\n",
    "\n",
    "evaluation_results = evaluator.evaluate_all_models(\n",
    "    final_models, \n",
    "    X_val, y_val, \n",
    "    groups=val_data['participant_id']\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Comprehensive Evaluation Complete\")\n",
    "print(f\"\\nEvaluation metrics computed: {len(list(evaluation_results.values())[0]['metrics'])}\")\n",
    "print(\"Metrics included: R², RMSE, MAE, MAPE, Explained Variance, and more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a45172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform participant-aware cross-validation\n",
    "print(\"Performing participant-aware cross-validation...\")\n",
    "\n",
    "cv_results = evaluator.participant_aware_cv(\n",
    "    final_models,\n",
    "    modeling_data[feature_columns],\n",
    "    modeling_data['ccr'],\n",
    "    groups=modeling_data['participant_id'],\n",
    "    cv_folds=5\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Cross-validation Complete\")\n",
    "print(\"\\nCV Performance Summary (mean ± std):\")\n",
    "for model_name, cv_scores in cv_results.items():\n",
    "    r2_mean = np.mean(cv_scores['r2'])\n",
    "    r2_std = np.std(cv_scores['r2'])\n",
    "    rmse_mean = np.mean(cv_scores['rmse'])\n",
    "    rmse_std = np.std(cv_scores['rmse'])\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  R²: {r2_mean:.4f} ± {r2_std:.4f}\")\n",
    "    print(f\"  RMSE: {rmse_mean:.4f} ± {rmse_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c41b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing\n",
    "print(\"Performing statistical significance testing...\")\n",
    "\n",
    "significance_results = evaluator.statistical_significance_testing(\n",
    "    cv_results, \n",
    "    baseline_model=best_baseline[0],\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Statistical Testing Complete\")\n",
    "print(\"\\nStatistical Significance Results:\")\n",
    "for comparison, result in significance_results.items():\n",
    "    print(f\"{comparison}:\")\n",
    "    print(f\"  p-value: {result['p_value']:.6f}\")\n",
    "    print(f\"  Significant: {result['significant']}\")\n",
    "    print(f\"  Effect size: {result['effect_size']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Phase 7 Complete: Comprehensive evaluation finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63738f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 8: Results Analysis and Reporting\n",
    "\n",
    "### Objectives:\n",
    "- Generate comprehensive evaluation report\n",
    "- Create performance visualizations\n",
    "- Analyze model interpretability\n",
    "- Prepare final results summary\n",
    "\n",
    "### Expected Outputs:\n",
    "- Detailed evaluation report\n",
    "- Performance comparison charts\n",
    "- Model interpretability analysis\n",
    "- Competition submission recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "print(\"Generating comprehensive evaluation report...\")\n",
    "\n",
    "report_generator = EvaluationReport()\n",
    "evaluation_report = report_generator.generate_comprehensive_report(\n",
    "    evaluation_results,\n",
    "    cv_results,\n",
    "    significance_results,\n",
    "    dataset_info={\n",
    "        'total_participants': modeling_data['participant_id'].nunique(),\n",
    "        'total_samples': len(modeling_data),\n",
    "        'feature_count': len(feature_columns),\n",
    "        'train_participants': train_data['participant_id'].nunique(),\n",
    "        'val_participants': val_data['participant_id'].nunique()\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ Evaluation report generated\")\n",
    "print(f\"✓ Report sections: {len(evaluation_report)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation report\n",
    "report_path = f\"results/evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(evaluation_report)\n",
    "\n",
    "print(f\"✓ Evaluation report saved to: {report_path}\")\n",
    "\n",
    "# Display key findings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best performing model\n",
    "all_final_results = {**evaluation_results, **{k: {'metrics': {'r2': np.mean(v['r2'])}} \n",
    "                                             for k, v in cv_results.items()}}\n",
    "best_model = max(all_final_results.items(), \n",
    "                key=lambda x: x[1]['metrics'].get('r2', x[1].get('val_r2', -float('inf'))))\n",
    "best_model_name, best_model_results = best_model\n",
    "\n",
    "print(f\"\\n🏆 BEST PERFORMING MODEL: {best_model_name}\")\n",
    "if 'metrics' in best_model_results:\n",
    "    print(f\"   R² Score: {best_model_results['metrics'].get('r2', 'N/A'):.4f}\")\n",
    "    print(f\"   RMSE: {best_model_results['metrics'].get('rmse', 'N/A'):.4f}\")\n",
    "    print(f\"   MAE: {best_model_results['metrics'].get('mae', 'N/A'):.4f}\")\n",
    "\n",
    "print(f\"\\n📊 DATASET STATISTICS:\")\n",
    "print(f\"   Total Participants: {modeling_data['participant_id'].nunique()}\")\n",
    "print(f\"   Total Samples: {len(modeling_data)}\")\n",
    "print(f\"   Features Engineered: {len(feature_columns)}\")\n",
    "print(f\"   Train/Val Split: {train_data['participant_id'].nunique()}/{val_data['participant_id'].nunique()} participants\")\n",
    "\n",
    "print(f\"\\n🔬 SIGNIFICANT IMPROVEMENTS:\")\n",
    "significant_models = [comp for comp, result in significance_results.items() \n",
    "                     if result['significant']]\n",
    "print(f\"   Models significantly better than baseline: {len(significant_models)}\")\n",
    "for model in significant_models[:5]:  # Show top 5\n",
    "    print(f\"   - {model}\")\n",
    "\n",
    "print(\"\\n✅ Phase 8 Complete: Results analysis and reporting finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84df181",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pipeline Execution Summary\n",
    "\n",
    "### 🎯 Mission Accomplished!\n",
    "\n",
    "The complete CGMacros CCR prediction pipeline for IEEE BHI 2025 Track 2 has been successfully executed. All phases have been completed with comprehensive results.\n",
    "\n",
    "### 📈 Next Steps:\n",
    "\n",
    "1. **Review Results**: Check the detailed evaluation report saved in the `results/` directory\n",
    "2. **Model Selection**: Use the best performing model for competition submission\n",
    "3. **Further Optimization**: Consider additional hyperparameter tuning or feature engineering\n",
    "4. **Visualization**: Run the comprehensive figure generation script for detailed visualizations\n",
    "\n",
    "### 💾 Saved Artifacts:\n",
    "\n",
    "- **Models**: Trained models saved in `models/` directory\n",
    "- **Results**: Evaluation reports in `results/` directory\n",
    "- **Data**: Processed datasets in `data/processed/` directory\n",
    "- **Figures**: Generated visualizations (if figure generation script was run)\n",
    "\n",
    "### 🏆 Competition Readiness:\n",
    "\n",
    "Your solution is now ready for the IEEE BHI 2025 Track 2 challenge submission. The best performing model and comprehensive evaluation results provide a strong foundation for your competition entry.\n",
    "\n",
    "---\n",
    "\n",
    "**Total Pipeline Execution Time**: Tracked across all phases  \n",
    "**Final Status**: ✅ COMPLETE - Ready for Competition Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
